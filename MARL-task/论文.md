一、论文要解决的问题（Problem）

论文面向自动化集装箱码头的连续泊位分配（continuous berth）与岸电协同分配，希望同时优化泊位利用率、船舶等待时间与碳排放，并在实时（秒级）输出可执行方案。作者指出传统数学规划/启发式方法在求解耗时、难以借鉴历史经验方面存在不足，因此采用多智能体强化学习（MARL）构建CTDE（集中训练、分散执行）框架解决部分可观测、多主体竞争下的联动调度。见摘要与引言（p2–3，图1）。

评审意见要点：问题定位清晰、工程价值高；“连续泊位+岸电”一体化建模具有现实意义。

二、采用的方法（Method）与原理（Principles）
2.1 建模：POMDP + 多智能体

将问题建模为POMDP，在CTDE范式下训练，执行时每艘船仅依据自身局部观测决策（Actor 局部，Critic 使用全局）。状态向量为18维局部特征（静态/动态船舶特征、岸电容量与使用量、泊位动态特征等），并拼接为全局特征矩阵供 Critic 评估（p5，图2）。动作包含船头靠泊位置 $l_n$、锚地等待时长 $t_n$、岸电使用概率 $𝜋_n$（p6）。约束覆盖空间安全距离、时间先后、岸电容量/覆盖等（p3）。

原理点评：POMDP + CTDE是应对多主体非平稳与部分可观测的标准做法；连续动作空间与“位置-时间-概率”决策十分贴合连续泊位场景。

2.2 学习算法：MATD3（双Critic、延迟策略）

主算法为MATD3，即 TD3 的多智能体变体。核心是双Critic取最小值抑制 Q 值高估、延迟更新 Actor提升稳定性（p8–9，图3–5；Alg.1–2）。

探索策略针对动作物理含义分维度设计：位置加入高斯噪声、等待时长用指数噪声、岸电概率用均匀噪声（式(17)–(19)，p9）。

原理点评：双Critic取最小值 + 延迟策略更新是 TD3 的关键；异质噪声与动作含义匹配，有助于更有效的探索。

2.3 奖励函数（解决奖励稀疏）

论文设计了5类奖励（基础正奖励、等待/碳排负奖励、使用岸电正奖励、泊位利用率全局奖励、无效动作惩罚），并给出具体公式（式(5)–(16)；表1，p6–7）。

原理点评：将多目标（经济/环保/效率）拆解为层次化即时奖励，有助于加速收敛并平衡目标。

三、实验设置与主要结论
3.1 实验环境与参数

仿真：2000 m 连续岸线、7天规划期、5段岸电覆盖；给出了岸电容量、碳排系数、惩罚/奖励权重 $𝑐_1∼𝑐_8
$	​等（表2，p10）。硬件/框架为 PyTorch，给出学习率、软更新、批量、经验池等超参（表3，p11）。

还提供了船舶生成算法（Algorithm 3，p10），按给定分布随机生成长度、到港时间、是否可用岸电等。

3.2 消融/敏感性

对Actor/Critic 学习率和软更新间隔做了敏感性分析（图6–8，p11–12），并通过三船稳定性与三船智能性测试验证算法可收敛且能应对不同到港属性（表4–5、图9–10，p12）。

双Critic 抑制 Q 高估：图11（p13）显示双Critic的 Q 估计更稳、更接近目标 Q，时序差分误差更低。

3.3 与基线比较（MADDPG、MAAC）

结果（表6，p14）：在65艘船高负载下，MATD3 的泊位利用率 46.72%，优于 MADDPG（44.54%）与 MAAC（45.71%）；碳排放显著低于 MAAC（62,853 kg vs 76,092 kg，≈–17.4%）；响应时间（前向一次）均为秒级，MATD3 在 65 艘时约 0.135 s（表7，p15；图12给出调度甘特图）。

评审总结：

模型在多项指标上优于基线，特别是降低碳排与提升利用率的平衡较好；策略网络推理实时性满足 TOS 在线调度需求。

四、分部分问题与修改建议（Reviewer comments, by section）

下面按论文结构逐段指出可以改进之处，并给出可操作建议。页码/图表位置便于作者定位修改。

4.1 摘要（p2）

问题

摘要声称“MATD3 训练时间更短”（见“实验结果表明…”段）与表6数据不一致：在多组规模上，MAAC 的训练时间更短（如 65 艘：MAAC 5798.93 min < MATD3 9468.70 min < MADDPG 11346.28 min）。

“平均等待时间减少 37%（相较于 MADDPG）”的数字未在主要表格中直观对应（65 艘时，MATD3 平均等待 323.81 vs MADDPG 405.03，约降 20%；其他规模亦未稳定达到 37%）。建议给出清晰口径（是某一优先级人群、还是某一规模下的最大降幅）。

建议

修正与表6不吻合的训练时间与降幅表述，避免过度声称；可改为“相较 MADDPG 更短，接近（或略高于）MAAC”、“在XX场景平均等待降幅可达 YY%”。



4.2 引言与相关工作（p2–3）

问题

对“连续泊位 + 岸电协同”的工程难点介绍很到位，但对MARL 在港口调度中的近期工作引用略少，且对MAAC/MADDPG 优劣讨论偏概述。

建议

增补最新 MARL 调度文献与策略学习可解释性/安全性讨论；并将本文贡献与元启发式、混合整数模型（MIP/CP）在可扩展性和近似最优保证方面进行更细粒度对比。

4.3 模型与符号（p4–6；图2）

问题

状态特征中引入“噪音”项（p5）以增强探索，这在观测中掺入噪声的做法较少见，可能影响同态复现实验与策略稳定性评估。

部分公式呈现不够清晰（如式(10) 全局利用率奖励的缩放与单位）；若无严格量纲化，可能带来奖励权重敏感问题。

建议

将“探索噪声”仅用于动作层（Actor 输出后加噪），观测向量保持确定性；或至少在附录解释该噪声项的统计特性与复现实验开关。

明确式(10)等奖励项的量纲与归一化，并在附录提供权重 $𝑐_1～ c_8$的消融和鲁棒性结果（±20% 变化对指标的影响曲线）。

4.4 奖励函数（表1、式(5)–(16)，p6–7）

问题

“分散靠泊奖励”式(9)的设计（对 $l/L$与 0.5 的距离给奖励或惩罚）可能隐式偏向岸线中心或两端，与“全局利用率最大化”的目标存在潜在耦合/冲突；实际更希望“空间均匀性”或“冲突最少”，而非对特定位置偏好。

建议

将式(9)替换为基于邻接拥挤度或覆盖均匀性的可微指标（如基于核密度或相邻船间距的凸惩罚），并补充有/无该项的对照实验。

4.5 算法设计与网络结构（图3–5；Alg.1–2，p7–9）

问题

训练流程完整，但尚未说明优化器（Adam/SGD）细节、目标延迟更新步数与随机种子。另外，将 MAAC/MADDPG 的网络结构强行与 MATD3 对齐（p14）可能削弱基线的原生优势（如 MAAC 的注意力机制）。

建议

在附录补充优化器/权重衰减/梯度裁剪等实现细节与随机种子；

对基线采用各自论文的推荐结构与超参，并做独立调参（网格或贝叶斯搜索），以确保基线公平；同时保留“统一结构”的对照，呈现双轨结果。

4.6 实验设置（表2–3；Algorithm 3，p10–11）

问题

船舶生成分布多为均匀/简单函数（如到港时间 $𝑈
(
0
,
𝑇
/
6$)、吃水与船长线性关联），与真实港口的季节性/峰谷/潮汐/一条航道通行约束差距较大。

碳排系数（船舶辅机 vs 岸电：2500 vs 800，表2）缺少来源与单位（kg/MWh?），可能影响“减排幅度”的客观性。

建议

增补真实港口数据或至少使用更贴近现实的到港过程（如非齐次泊松/双对数正态间隔），并加入潮汐、单/双向航道、拖轮/引航时间窗等约束的敏感性测试；

给出碳排系数的文献出处与单位，并考虑电网时变排放因子/电价的情景分析。

4.7 结果与可视化（表4–7；图9–12，p12–15）

问题

训练时间的结论与表6不符（见 4.1）；

表6中的“等待时间 p4/p3/p2/p1”列含大量 0，口径不明（是否分优先级均值？是否仅统计等待>0？）。

图12甘特图信息量大，但可以辅助冲突检测（重叠、最小间隔）与岸电使用率的曲线更直观。

建议

统一口径后重述结论；

在正文明确“等待时间 p4–p1”的定义与统计规则；

增加岸电时序负载曲线、泊位占用率热力图、冲突率等可视化，提升读者对方案质量的直观判断。

4.8 消融与鲁棒性

问题

已做学习率与软更新的敏感性，但对奖励权重、探索噪声尺度、岸电容量/覆盖分段等关键因素缺少系统消融。

建议

增补对$𝑐1∼𝑐8$ ,$𝜎_{𝑛𝑜𝑖𝑠𝑒}$与岸电段数/容量的敏感性；报告对收敛速度、稳定性、指标的影响，提升结论的普适性。

4.9 讨论与限制（结论 p15）

问题

结论处仅简要提到“未考虑潮汐”作为局限，实际上航道单向/拥堵、岸桥/场桥/AGV联动、电网约束等亦是影响联动调度的现实因素。

建议

在“局限与展望”中系统罗列尚未纳入的工程约束，并给出扩展到‘泊位–岸电–岸桥–水平运输–堆场’的端到端联合学习路线（分层MARL或层级RL），增强论文的前瞻性与说服力。

五、总体评价与建议结论

创新性：算法本体（MATD3）非新，但将其针对“连续泊位+岸电协同”的POMDP建模、层次奖励与异质探索做了面向工程的设计，属方法工程创新，有实用价值。

正确性/充分性：建模与实现路径合理，实验覆盖到不同规模，双Critic 抑制 Q 高估的证据充分（图11）。但训练时间结论与表格不一致，基线公平性与现实数据/约束仍需加强。

可复现性：核心超参已给出，但需补充代码/随机种子/优化器细节与数据生成脚本，建议开源。

实际影响：策略前向秒级，适合接入 TOS 做滚动重算；若引入真实到港与电网/航道约束，潜在工程落地价值高。

综合建议：大修（Major revision）后接收。重点请：

统一并纠正与表格不一致的结论陈述（训练时间、降幅）；

加强基线公平性（按各自最佳实践调参/结构）；

补充现实约束与数据（或更逼真的到港/电网/潮汐模型）及奖励/噪声消融；

完善可复现性（实现细节与代码/数据）。

六、可直接加入稿件的增补材料清单（便于修改）

方法：给出式(10)等奖励项量纲与归一化说明；将“观测噪声”改为“动作噪声”，并说明原因与影响。

实验：

新增“现实感更强”的到港过程与电网排放情景；

对 $𝑐1∼𝑐8$做灵敏度曲线（±20%、±50%）；

基线算法采用原生结构+独立调参，并与“统一结构”做双轨对比；

补充图表：岸电时序负载曲线、泊位占用热力图、冲突率。

文本：修订摘要与结论中的训练时间/降幅表述；在“局限与展望”中扩展潮汐/航道/设备联动的研究计划。

附：论文关键信息速览（按页码）

图1（p3）给出连续岸线与岸电覆盖示意；

状态/动作/奖励定义（p5–7，表1、式(5)–(16)）；

算法框架（图3–5；Alg.1–2，p7–9）；探索噪声（式(17)–(19)，p9）；

环境/超参（表2–3，p10–11）；敏感性（图6–8，p11–12）；

稳定性与智能性（表4–5、图9–10，p12）；Q高估抑制（图11，p13）；

对比结果（表6、图12，p14）；响应时间（表7，p15）；结论与局限（p15）。

论文有创新性，但主要属于工程/系统层面的集成与落地创新，方法论层面（算法原理）创新有限。更准确的定位应是“将成熟的 MARL（MATD3+CTDE）做成适用于‘连续泊位+岸电协同’的可实用框架”，并通过特定的状态表示、奖励塑形与探索机制，在仿真中取得了不错的多目标权衡与秒级推理时间。

具体“新”在何处（正向评价）

问题整合与场景选择：把“连续泊位”（连续空间的靠泊位置）与“岸电协同”（容量/覆盖段限制）合在一个 POMDP+CTDE 的 MARL 框架中求解；既保持了连续决策空间，又显式处理岸电段容量与覆盖约束。这与文献中更常见的离散泊位+启发式/数规路线不同（引言对两条研究路径及其局限有系统对比，p2–3；图1展示了连续岸线与岸电覆盖带，p3）。

状态—动作—奖励的面向工程设计：

全局–局部特征表示：为每艘船构造 18 维局部特征，并拼成全局特征矩阵供 Critic 使用（图2，p5），使 CTDE 在该多主体调度中可落地。

结构化奖励塑形：将经济性/环保/效率三目标拆成 5 类奖励（含“泊位利用率全局项”“岸电使用奖励”“无效动作惩罚”等），缓解稀疏奖励并加快收敛（式(5)–(16)，表1，p6–7）。这种面向场景的奖励分解与量纲化呈现，是本工作可复用的工程贡献。

维度感知的探索噪声：针对“位置/时间/概率”三类动作，分别采用高斯/指数/均匀噪声（式(17)–(19)，p9），与动作物理意义匹配，提升探索效率。

稳定性与可用性证据：

在本场景下实证展示双 Critic 抑制 Q 高估、更稳的 TD 误差（图11，p13），把经典原理在港口调度域内做了可视化与量化证明。

前向推理秒级（65 艘时 ≈0.135 s，表7，p15），对 TOS 的在线滚动重算具有现实意义。

小结：这些点并非提出“新算法”，而是把通用算法工匠化到复杂工程问题上，形成可复用的方法工程范式。

哪些不新（或创新性有限）

算法内核并不新：MATD3、双 Critic 与延迟更新、CTDE/POMDP 都是成熟做法；本文没有在理论或收敛性上给出新的定理或算法机制。相关工作中亦已有将泊位分配建模为 RL/MDP 的探索（引言引用 [12]–[15] 等），只是多为离散或未与岸电连续耦合。

基线设置趋同：为“公平”将 MAAC/MADDPG 的网络结构与超参对齐到本框架（p14），可能弱化了基线原生优势，不利于证明“方法层面的显著创新”。

若干结论表述与数据不完全一致（影响说服力而非“创新性”本身）：摘要称“训练时间更短”，但表6显示在部分规模 MAAC 训练更短（如 65 艘：5798.93 min < 9468.70 min），这会削弱稿件对“效率优势”的原创性/实证力度。

综述式判断与定位建议

创新性等级：中等（工程/系统创新为主，方法创新有限）。论文把连续岸线+岸电容量/覆盖段的复杂约束，以可训练、可推理、可运行的 MARL 方案落地，并给出较完整的实验与可视化证据。和“提出新算法/新理论”相比，更像一篇高质量的应用方法论文。

建议的贡献表述（可放到“贡献/摘要”处，避免过度声称）：

首次/系统性地将连续泊位与岸电协同建模为 POMDP 下的 CTDE-MARL 并实现秒级在线推理；

提出全局–局部状态表示 + 结构化奖励 + 维度感知探索的可复用设计范式，在多目标（等待/碳排/利用率）间取得稳定权衡；

在该域内实证验证双 Critic 抑制 Q 高估及奖励塑形的有效性，并给出端到端仿真对比与可视化。
这些表述更贴近论文的实际贡献形态与强项。

如何进一步“做新做深”（供大修或延伸研究参考）：

引入真实到港/电网时变排放/潮汐与航道约束等现实要素做情景对比，增强外部效度；

对奖励权重/噪声尺度/岸电段数与容量做系统消融，证明设计的普适性；

与原生配置的 MAAC/MADDPG、以及代表性MIP/元启发式做双轨公平对比，突出工程范式的稳健优势；

探索分层/多层次 MARL，把“泊位–岸电”扩展到“岸桥/水平运输/堆场”的端到端协同，形成进一步的系统级创新。


1) 论文哪里写到“状态里加噪音用于增强探索”？

正文 1.3.1「状态空间」第（6）点
原文在“状态空间”的 18 维特征列表里，第（6）条明确写道：
“（6）噪音，目的是增强智能体的探索。”
这段在正文 Page 4/15（PDF 全文页码为第 5 页）。

图 2（局部/全局特征矩阵）中的标注
在“局部和全局状态特征”图示里，右下角的特征示意旁也直标“噪音”，对应状态向量中的这一维。该图在正文 Page 5/15（PDF 全文页码第 6 页）。

以上两处共同表明：作者确实把一个“噪音”维度并入了状态特征，并在文字里将其目的描述为“增强探索”。

另外，论文还专门对动作端的探索噪声做了说明（不是状态端）：

2.3.2「动作合探索策略」
明确给出 式(17)–(19)：靠泊位置加高斯噪声、等待时长加指数噪声、岸电概率加均匀噪声，用于探索动作空间。该段在正文 Page 9/15（PDF 全文页码第 10 页）。

2) “在感知/状态里加噪声”到底算不算“随机化”？与“在不确定感知下如何合理决策”是什么关系？

是否属于随机化
把随机噪声作为状态的一部分（或对观测施加扰动），在方法论上属于一种显式随机化/域随机化（domain randomization）或输入层噪声增强。它的直接效果是让策略在训练时接触更“嘈杂”的感知分布，从而提高鲁棒性/泛化。但这与“探索”不是一回事：

探索通常指动作端的随机性（如 ϵ-greedy、OU/高斯噪声等），目标是更全面地尝试不同动作；

观测端加噪更多是训练时的数据增强或环境不确定性的模拟，主要影响泛化和抗干扰，而非直接驱动动作空间的探索。因此，把“状态里加噪声”称为“增强探索”，表述并不严谨；若目的确实是探索，动作噪声（论文式(17)–(19)那部分）才是正统做法。

在感知不确定下如何做出“合理决策”,这属于POMDP语境：感知不完美/有噪声时，合理决策常见做法是

显式建模不确定性：如用RNN/GRU做信念状态估计（用历史观测压缩成隐变量），或用滤波/集合方法近似后验；

风险/鲁棒目标：采用风险敏感或分布鲁棒目标；

输入扰动的鲁棒训练：可在训练时对观测添加真实感的测量噪声，但不需要把“噪音”做成单独特征维；

不确定性感知的策略：用dropout/ensemble近似不确定度，并在决策时考虑不确定性惩罚。
这些才是“在不确定感知下做出更合理决策”的标准路径；单独在状态向量拼接一个‘噪音维’更像是随机化增强，不是严格的信念建模。

3) 给本文的具体建议（可直接用于修改说明）

把“状态里的噪音”用途表述从“增强探索”改为“提高鲁棒性/泛化的输入扰动”，并补做消融（去掉该维 vs 保留该维），比较收敛速度、等待/碳排/利用率等指标；在附录说明该噪声分布与幅度。

探索仍建议走“动作噪声”路线（论文已在式(17)–(19)实现），与“状态噪声”明确区分，避免概念混淆。

若目标是**“感知不确定下的合理决策”，建议在方法节补充POMDP 的信念状态/历史聚合实现（如在 Actor 前加一层时序编码），并用带观测噪声的场景**做鲁棒性测试。

一句话结论

论文确实在状态特征里加入了“噪音”并写明“用于增强探索”（正文 Page 4/15；图 2 于 Page 5/15）；动作端的噪声另行在**式(17)–(19)**给出（Page 9/15）。就概念而言，状态端噪声属于随机化/数据增强，更对鲁棒性有益，不等价于动作层面的“探索”，也不等同于对感知不确定性的规范建模。

证据位置（论文原文）
在**第 3.4 节“强化学习算法对比”**中，紧接 Fig.11 之后、表 6 之前，作者明确写道：

“为保障对比实验的公平性，将 MAAC、MADDPG 算法的 Critic 结构、Actor 结构分别与 MATD3 算法的 Critic 结构（图4）、Actor 结构（图5）设置一致，超参数均按表5设置。两类算法的更新规则等可参考文献[18][19]。” 

该段位于 PDF 第 13 页（系统页码 13/16），章节标题为“3.4 强化学习算法对比”。从语义上这就是把 MAAC/MADDPG 的 Actor/Critic 架构“对齐”为与 MATD3 一致（即使用图4与图5所示的网络），这会弱化 MAAC 原生的注意力机制等结构特征，属于您所说的“强行对齐”。

注：同页还提到“超参数均按表5设置”，但文中表 5 实际是“三艘船舶信息”，这里也存在表号引用不一致的问题。