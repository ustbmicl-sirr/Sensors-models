# ç®—æ³•è®¾è®¡å®Œæ•´æ–‡æ¡£

**å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ³Šä½åˆ†é…ä¸å²¸ç”µååŒä¼˜åŒ–**

---

## ğŸ“‹ ç›®å½•

- [ç¬¬ä¸€éƒ¨åˆ†ï¼šé—®é¢˜å»ºæ¨¡](#ç¬¬ä¸€éƒ¨åˆ†é—®é¢˜å»ºæ¨¡)
- [ç¬¬äºŒéƒ¨åˆ†ï¼šç¯å¢ƒè®¾è®¡](#ç¬¬äºŒéƒ¨åˆ†ç¯å¢ƒè®¾è®¡)
- [ç¬¬ä¸‰éƒ¨åˆ†ï¼šå¥–åŠ±å‡½æ•°](#ç¬¬ä¸‰éƒ¨åˆ†å¥–åŠ±å‡½æ•°)
- [ç¬¬å››éƒ¨åˆ†ï¼šç®—æ³•å®ç°](#ç¬¬å››éƒ¨åˆ†ç®—æ³•å®ç°)
- [ç¬¬äº”éƒ¨åˆ†ï¼šæ”¹è¿›ä¸ä¼˜åŒ–](#ç¬¬äº”éƒ¨åˆ†æ”¹è¿›ä¸ä¼˜åŒ–)

---

## ç¬¬ä¸€éƒ¨åˆ†ï¼šé—®é¢˜å»ºæ¨¡

### 1.1 é—®é¢˜æè¿°

**æ ¸å¿ƒç›®æ ‡**: ä¸ºåˆ°æ¸¯èˆ¹èˆ¶æ™ºèƒ½åˆ†é…æ³Šä½ä½ç½®ã€é æ³Šæ—¶é—´å’Œå²¸ç”µä½¿ç”¨ï¼Œå®ç°å¤šç›®æ ‡ä¼˜åŒ–

**ä¼˜åŒ–ç›®æ ‡**:
1. æœ€å°åŒ–èˆ¹èˆ¶ç­‰å¾…æ—¶é—´
2. æœ€å¤§åŒ–æ³Šä½åˆ©ç”¨ç‡
3. é™ä½ç¢³æ’æ”¾ (é¼“åŠ±ä½¿ç”¨å²¸ç”µ)
4. é¿å…èˆ¹èˆ¶æ‹¥æŒ¤
5. æ»¡è¶³è¿è¥çº¦æŸ

### 1.2 MARLå»ºæ¨¡

**å»ºæ¨¡æ¡†æ¶**: éƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (POMDP)

**å…³é”®è¦ç´ **:
- **æ™ºèƒ½ä½“**: æ¯è‰˜èˆ¹æ˜¯ä¸€ä¸ªç‹¬ç«‹æ™ºèƒ½ä½“
- **åŠ¨ä½œ**: è¿ç»­åŠ¨ä½œ [æ³Šä½ä½ç½®, ç­‰å¾…æ—¶é—´, å²¸ç”µæ¦‚ç‡]
- **è§‚æµ‹**: å±€éƒ¨è§‚æµ‹ (17ç»´ç‰¹å¾)
- **å¥–åŠ±**: å¤šç›®æ ‡å¥–åŠ±å‡½æ•°
- **ç»ˆæ­¢**: æ‰€æœ‰èˆ¹èˆ¶å®Œæˆé æ³Š

**CTDEæ¶æ„** (Centralized Training, Decentralized Execution):
- è®­ç»ƒæ—¶: ä½¿ç”¨å…¨å±€ä¿¡æ¯
- æ‰§è¡Œæ—¶: åŸºäºå±€éƒ¨è§‚æµ‹å†³ç­–

---

## ç¬¬äºŒéƒ¨åˆ†ï¼šç¯å¢ƒè®¾è®¡

### 2.1 è§‚æµ‹ç©ºé—´ (17ç»´)

```python
observation_space = Box(low=-1.0, high=1.0, shape=(17,), dtype=np.float32)
```

**ç»´åº¦è¯´æ˜**:

| ç»´åº¦ | ç‰¹å¾ | æè¿° | å½’ä¸€åŒ–æ–¹æ³• |
|------|------|------|-----------|
| 0 | æ³Šä½ä½ç½® | å½“å‰åˆ†é…çš„ä½ç½® | [0, berth_length] â†’ [-1, 1] |
| 1 | ç­‰å¾…æ—¶é—´ | å·²ç­‰å¾…æ—¶é—´ | [0, max_waiting] â†’ [-1, 1] |
| 2 | å²¸ç”µæ¦‚ç‡ | å²¸ç”µä½¿ç”¨æ¦‚ç‡ | [0, 1] â†’ [-1, 1] |
| 3 | èˆ¹èˆ¶é•¿åº¦ | èˆ¹é•¿ | [min_len, max_len] â†’ [-1, 1] |
| 4 | åˆ°æ¸¯æ—¶é—´ | åˆ°è¾¾æ—¶é—´ | [0, horizon] â†’ [-1, 1] |
| 5 | ä¼˜å…ˆçº§ | èˆ¹èˆ¶ä¼˜å…ˆçº§ | [0, 1] â†’ [-1, 1] |
| 6-10 | å²¸ç”µè´Ÿè½½ | 5æ®µå²¸ç”µä½¿ç”¨ç‡ | [0, capacity] â†’ [-1, 1] |
| 11 | æ³Šä½åˆ©ç”¨ç‡ | å½“å‰åˆ©ç”¨ç‡ | [0, 1] â†’ [-1, 1] |
| 12 | å¹³å‡ç­‰å¾… | å¹³å‡ç­‰å¾…æ—¶é—´ | [0, max_waiting] â†’ [-1, 1] |
| 13 | ç¢³æ’æ”¾ | ç´¯è®¡æ’æ”¾é‡ | [0, max_emission] â†’ [-1, 1] |
| 14 | å²¸ç”µä½¿ç”¨ç‡ | å²¸ç”µä½¿ç”¨æ¯”ä¾‹ | [0, 1] â†’ [-1, 1] |
| 15 | æˆåŠŸç‡ | åˆ†é…æˆåŠŸç‡ | [0, 1] â†’ [-1, 1] |
| 16 | æ—¶é—´æ­¥ | å½“å‰æ—¶é—´æ­¥ | [0, max_steps] â†’ [-1, 1] |

**è§‚æµ‹è·å–**:
```python
def _get_obs(self, vessel_id):
    """è·å–æ™ºèƒ½ä½“è§‚æµ‹"""
    vessel = self.vessels[vessel_id]

    obs = np.array([
        # å½“å‰åŠ¨ä½œçŠ¶æ€ (0-2)
        normalize(vessel.position, 0, self.berth_length),
        normalize(vessel.waiting_time, 0, self.max_waiting),
        vessel.shore_power_prob,

        # èˆ¹èˆ¶ç‰¹å¾ (3-5)
        normalize(vessel.length, self.min_vessel_len, self.max_vessel_len),
        normalize(vessel.arrival_time, 0, self.planning_horizon),
        vessel.priority,

        # å²¸ç”µçŠ¶æ€ (6-10)
        *[normalize(load, 0, capacity) for load, capacity in
          zip(self.shore_power.loads, self.shore_power.capacities)],

        # å…¨å±€æŒ‡æ ‡ (11-15)
        self.berth_utilization,
        normalize(self.avg_waiting_time, 0, self.max_waiting),
        normalize(self.total_emissions, 0, self.max_emissions),
        self.shore_power_usage_rate,
        self.allocation_success_rate,

        # æ—¶é—´è¿›åº¦ (16)
        normalize(self.current_step, 0, self.max_steps),
    ], dtype=np.float32)

    return np.clip(obs, -1.0, 1.0)
```

### 2.2 åŠ¨ä½œç©ºé—´ (3ç»´è¿ç»­)

```python
action_space = Box(low=-1.0, high=1.0, shape=(3,), dtype=np.float32)
```

**åŠ¨ä½œç»´åº¦**:

| ç»´åº¦ | åŠ¨ä½œ | åŸå§‹èŒƒå›´ | æ˜ å°„æ–¹æ³• |
|------|------|---------|----------|
| 0 | æ³Šä½ä½ç½® | [0, berth_length] | `(a[0]+1)/2 * berth_length` |
| 1 | ç­‰å¾…æ—¶é—´ | [0, max_waiting] | `(a[1]+1)/2 * max_waiting` |
| 2 | å²¸ç”µæ¦‚ç‡ | [0, 1] | `(a[2]+1)/2` |

**åŠ¨ä½œæ˜ å°„**:
```python
def map_action(self, raw_action):
    """å°†[-1,1]åŠ¨ä½œæ˜ å°„åˆ°å®é™…èŒƒå›´"""
    position = (raw_action[0] + 1) / 2 * self.berth_length
    waiting_time = (raw_action[1] + 1) / 2 * self.max_waiting_time
    shore_power_prob = (raw_action[2] + 1) / 2

    return {
        'position': np.clip(position, 0, self.berth_length),
        'waiting_time': np.clip(waiting_time, 0, self.max_waiting_time),
        'shore_power_prob': np.clip(shore_power_prob, 0, 1),
    }
```

**åŠ¨ä½œçº¦æŸ**:
```python
def check_action_valid(self, vessel, action):
    """æ£€æŸ¥åŠ¨ä½œæ˜¯å¦æœ‰æ•ˆ"""
    position, waiting_time, _ = action

    # çº¦æŸ1: ä½ç½®åœ¨æ³Šä½èŒƒå›´å†…
    if position < 0 or position + vessel.length > self.berth_length:
        return False

    # çº¦æŸ2: ç­‰å¾…æ—¶é—´ä¸è¶…è¿‡é™åˆ¶
    if waiting_time > self.max_waiting_time:
        return False

    # çº¦æŸ3: ä¸ä¸å…¶ä»–èˆ¹å†²çª
    for other_vessel in self.allocated_vessels:
        if self._check_collision(vessel, other_vessel, position):
            return False

    return True
```

### 2.3 èˆ¹èˆ¶ç”Ÿæˆ

**éé½æ¬¡æ³Šæ¾è¿‡ç¨‹**:
```python
class VesselGenerator:
    def generate_realistic(self, num_vessels, horizon_days):
        """ç”ŸæˆçœŸå®èˆ¹èˆ¶åºåˆ—"""
        vessels = []

        # éé½æ¬¡æ³Šæ¾è¿‡ç¨‹ - å¤šå³°åˆ°æ¸¯
        for t in range(horizon_days * 24):  # æŒ‰å°æ—¶
            # åˆ°æ¸¯ç‡éšæ—¶é—´å˜åŒ–
            rate = self._get_arrival_rate(t)

            # ç”Ÿæˆæ³Šæ¾æ•°é‡
            num_arrivals = np.random.poisson(rate)

            for _ in range(num_arrivals):
                vessel = self._create_vessel(t)
                vessels.append(vessel)

        return vessels[:num_vessels]

    def _get_arrival_rate(self, hour):
        """åˆ°æ¸¯ç‡å‡½æ•° (å¤šå³°)"""
        # æ—©é«˜å³° (6-10æ—¶)
        peak1 = 2.0 if 6 <= hour % 24 <= 10 else 0.5

        # æ™šé«˜å³° (18-22æ—¶)
        peak2 = 1.5 if 18 <= hour % 24 <= 22 else 0.5

        return peak1 + peak2

    def _create_vessel(self, arrival_time):
        """åˆ›å»ºèˆ¹èˆ¶"""
        # å¤šå³°èˆ¹é•¿åˆ†å¸ƒ
        vessel_type = np.random.choice(['small', 'medium', 'large'],
                                      p=[0.3, 0.5, 0.2])

        if vessel_type == 'small':
            length = np.random.normal(50, 10)
        elif vessel_type == 'medium':
            length = np.random.normal(150, 20)
        else:  # large
            length = np.random.normal(300, 30)

        return Vessel(
            length=np.clip(length, 30, 400),
            arrival_time=arrival_time,
            priority=np.random.uniform(0, 1),
            shore_power_capable=np.random.rand() > 0.3,
        )
```

### 2.4 å²¸ç”µç®¡ç†

```python
class ShorePowerManager:
    def __init__(self, num_segments=5):
        """5æ®µå²¸ç”µç®¡ç†"""
        self.num_segments = num_segments
        self.segment_length = 2000 / num_segments  # 400m/æ®µ

        # æ¯æ®µå®¹é‡ (kW)
        self.capacities = [500, 500, 500, 500, 500]
        self.loads = [0] * num_segments

    def allocate(self, vessel, position, shore_power_prob):
        """åˆ†é…å²¸ç”µ"""
        # ç¡®å®šèˆ¹èˆ¶å ç”¨å“ªäº›æ®µ
        start_seg = int(position / self.segment_length)
        end_seg = int((position + vessel.length) / self.segment_length)

        # æ£€æŸ¥å®¹é‡
        for seg in range(start_seg, end_seg + 1):
            if seg < self.num_segments:
                required = vessel.power_demand * shore_power_prob
                if self.loads[seg] + required > self.capacities[seg]:
                    return False, 0

        # åˆ†é…æˆåŠŸ
        allocated_power = 0
        for seg in range(start_seg, end_seg + 1):
            if seg < self.num_segments:
                power = vessel.power_demand * shore_power_prob
                self.loads[seg] += power
                allocated_power += power

        return True, allocated_power

    def calculate_emissions(self, vessel, shore_power_used):
        """è®¡ç®—ç¢³æ’æ”¾"""
        # ä½¿ç”¨å²¸ç”µéƒ¨åˆ†: è¾ƒä½æ’æ”¾
        shore_power_emission = shore_power_used * 0.5  # kg CO2/kWh

        # æœªä½¿ç”¨å²¸ç”µ: èˆ¹èˆ¶è‡ªå‘ç”µ
        remaining_power = vessel.power_demand - shore_power_used
        vessel_emission = remaining_power * 0.8  # kg CO2/kWh

        total = shore_power_emission + vessel_emission
        reduction = vessel.power_demand * 0.8 - total  # å‡æ’é‡

        return total, reduction
```

---

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šå¥–åŠ±å‡½æ•°

### 3.1 å¤šç›®æ ‡å¥–åŠ±è®¾è®¡

```python
reward = c1 * base_reward          # æˆåŠŸé æ³ŠåŸºç¡€å¥–åŠ±
       - c2 * waiting_penalty      # ç­‰å¾…æ—¶é—´æƒ©ç½š
       - c3 * emission_penalty     # ç¢³æ’æ”¾æƒ©ç½š
       + c4 * shore_power_bonus    # å²¸ç”µä½¿ç”¨å¥–åŠ±
       + c5 * utilization_reward   # æ³Šä½åˆ©ç”¨ç‡å¥–åŠ±
       + c6 * spacing_reward       # åˆ†æ•£é æ³Šå¥–åŠ±
```

**é»˜è®¤æƒé‡**:
```python
reward_weights = {
    'c1': 1.0,   # åŸºç¡€å¥–åŠ±
    'c2': 0.5,   # ç­‰å¾…æƒ©ç½š
    'c3': 0.3,   # æ’æ”¾æƒ©ç½š
    'c4': 0.4,   # å²¸ç”µå¥–åŠ±
    'c5': 0.6,   # åˆ©ç”¨ç‡å¥–åŠ±
    'c6': 0.2,   # åˆ†æ•£å¥–åŠ±
}
```

### 3.2 å¥–åŠ±åˆ†é¡¹è¯¦è§£

#### 3.2.1 åŸºç¡€å¥–åŠ±
```python
def base_reward(self, vessel):
    """æˆåŠŸé æ³Šçš„åŸºç¡€å¥–åŠ±"""
    if self.is_allocated_successfully(vessel):
        # æ ¹æ®ä¼˜å…ˆçº§åŠ æƒ
        reward = 10.0 * vessel.priority
        return reward
    else:
        return 0.0
```

#### 3.2.2 ç­‰å¾…æ—¶é—´æƒ©ç½š
```python
def waiting_penalty(self, vessel):
    """ç­‰å¾…æ—¶é—´æƒ©ç½š (å‡¸å‡½æ•°)"""
    waiting_hours = vessel.waiting_time

    # åˆ†æ®µæƒ©ç½š
    if waiting_hours <= 2:
        penalty = waiting_hours * 0.5
    elif waiting_hours <= 6:
        penalty = 1.0 + (waiting_hours - 2) * 1.0
    else:
        penalty = 5.0 + (waiting_hours - 6) * 2.0

    return penalty
```

#### 3.2.3 ç¢³æ’æ”¾æƒ©ç½š
```python
def emission_penalty(self, vessel, shore_power_used):
    """ç¢³æ’æ”¾æƒ©ç½š"""
    total_emission, reduction = self.shore_power.calculate_emissions(
        vessel, shore_power_used
    )

    # å½’ä¸€åŒ–æ’æ”¾é‡
    normalized_emission = total_emission / vessel.max_possible_emission

    penalty = normalized_emission * 5.0
    return penalty
```

#### 3.2.4 å²¸ç”µä½¿ç”¨å¥–åŠ±
```python
def shore_power_bonus(self, vessel, shore_power_used):
    """å²¸ç”µä½¿ç”¨å¥–åŠ±"""
    # ä½¿ç”¨æ¯”ä¾‹
    usage_ratio = shore_power_used / vessel.power_demand

    # éçº¿æ€§å¥–åŠ± (é¼“åŠ±é«˜ä½¿ç”¨ç‡)
    bonus = usage_ratio ** 2 * 3.0

    return bonus
```

#### 3.2.5 æ³Šä½åˆ©ç”¨ç‡å¥–åŠ±
```python
def utilization_reward(self):
    """æ³Šä½åˆ©ç”¨ç‡å¥–åŠ±"""
    # æ—¶ç©ºåˆ©ç”¨ç‡
    utilization = self.calculate_berth_utilization()

    # ç›®æ ‡åˆ©ç”¨ç‡: 80-90%
    if 0.8 <= utilization <= 0.9:
        reward = 5.0
    elif utilization > 0.9:
        # è¿‡é«˜åˆ©ç”¨ç‡è½»å¾®æƒ©ç½š (å¯èƒ½æ‹¥æŒ¤)
        reward = 5.0 - (utilization - 0.9) * 10.0
    else:
        # ä½åˆ©ç”¨ç‡æƒ©ç½š
        reward = utilization * 6.25  # 0.8*6.25=5.0

    return reward
```

#### 3.2.6 åˆ†æ•£é æ³Šå¥–åŠ± (æ”¹è¿›ç‰ˆ)
```python
def spacing_reward(self, vessel, position):
    """åˆ†æ•£é æ³Šå¥–åŠ± - åŸºäºé‚»è¿‘æ‹¥æŒ¤åº¦"""
    # åŸç‰ˆ: åŸºäºå²¸çº¿ä¸­å¿ƒè·ç¦» (ä¸åˆç†)
    # æ”¹è¿›: åŸºäºé‚»è¿‘èˆ¹èˆ¶æ‹¥æŒ¤åº¦

    # è®¡ç®—é‚»è¿‘æ‹¥æŒ¤åº¦
    neighbor_density = 0
    search_radius = 200  # 200mèŒƒå›´å†…

    for other_vessel in self.allocated_vessels:
        distance = abs(other_vessel.position - position)

        if distance < search_radius:
            # è·ç¦»è¶Šè¿‘,æ‹¥æŒ¤åº¦è¶Šé«˜
            density_contribution = (search_radius - distance) / search_radius
            neighbor_density += density_contribution

    # æ‹¥æŒ¤åº¦è¶Šä½,å¥–åŠ±è¶Šé«˜
    reward = max(0, 2.0 - neighbor_density)

    return reward
```

### 3.3 å®Œæ•´å¥–åŠ±è®¡ç®—

```python
class RewardCalculator:
    def __init__(self, weights):
        self.weights = weights

    def calculate_reward(self, vessel, action, state):
        """è®¡ç®—å®Œæ•´å¥–åŠ±"""
        position, waiting_time, shore_power_prob = action

        # æ£€æŸ¥åŠ¨ä½œæœ‰æ•ˆæ€§
        if not self.is_valid_action(vessel, action):
            return -10.0  # æ— æ•ˆåŠ¨ä½œå¤§æƒ©ç½š

        # åˆ†é¡¹è®¡ç®—
        r1 = self.base_reward(vessel)
        r2 = self.waiting_penalty(vessel)
        r3 = self.emission_penalty(vessel, shore_power_prob)
        r4 = self.shore_power_bonus(vessel, shore_power_prob)
        r5 = self.utilization_reward()
        r6 = self.spacing_reward(vessel, position)

        # åŠ æƒæ±‚å’Œ
        total_reward = (
            self.weights['c1'] * r1
            - self.weights['c2'] * r2
            - self.weights['c3'] * r3
            + self.weights['c4'] * r4
            + self.weights['c5'] * r5
            + self.weights['c6'] * r6
        )

        return total_reward

    def get_reward_breakdown(self):
        """è¿”å›å¥–åŠ±åˆ†è§£ (ç”¨äºåˆ†æ)"""
        return {
            'base_reward': self.last_r1,
            'waiting_penalty': self.last_r2,
            'emission_penalty': self.last_r3,
            'shore_power_bonus': self.last_r4,
            'utilization_reward': self.last_r5,
            'spacing_reward': self.last_r6,
            'total': self.last_total,
        }
```

---

## ç¬¬å››éƒ¨åˆ†ï¼šç®—æ³•å®ç°

### 4.1 SACç®—æ³• (æ¨è)

**ç®—æ³•ç‰¹ç‚¹**:
- æœ€å¤§ç†µå¼ºåŒ–å­¦ä¹ 
- Off-policy
- Twin Q-networks
- è‡ªåŠ¨æ¸©åº¦è°ƒèŠ‚
- é€‚åˆè¿ç»­åŠ¨ä½œç©ºé—´

**æ ¸å¿ƒå…¬å¼**:

**ç­–ç•¥ç›®æ ‡**:
```
J(Ï€) = E[âˆ‘ Î³^t (r_t + Î± H(Ï€(Â·|s_t)))]

H(Ï€) = -E[log Ï€(a|s)]  # ç†µ
```

**Qå‡½æ•°æ›´æ–°**:
```
Q(s,a) â† r + Î³ min(Qâ‚(s',a'), Qâ‚‚(s',a'))  # Twin Q
```

**ç­–ç•¥æ›´æ–°**:
```
Ï€ â† arg max E[Q(s,a) - Î± log Ï€(a|s)]
```

**RLlibé…ç½®**:
```python
from ray.rllib.algorithms.sac import SACConfig

config = SACConfig()

config.training(
    # Replay buffer
    train_batch_size=256,
    replay_buffer_config={
        "type": "MultiAgentReplayBuffer",
        "capacity": 500000,
    },

    # Optimization
    optimization={
        "actor_learning_rate": 3e-4,
        "critic_learning_rate": 3e-4,
        "entropy_learning_rate": 3e-4,
    },

    # SAC specific
    tau=0.005,                    # è½¯æ›´æ–°ç³»æ•°
    target_network_update_freq=1,
    twin_q=True,                  # åŒQç½‘ç»œ
    policy_delay=2,               # å»¶è¿Ÿç­–ç•¥æ›´æ–°

    # æ¢ç´¢
    initial_alpha=1.0,
    target_entropy="auto",        # è‡ªåŠ¨è°ƒèŠ‚æ¸©åº¦
)

# ç½‘ç»œæ¶æ„
config.model = {
    "fcnet_hiddens": [256, 256],
    "fcnet_activation": "relu",
}
```

### 4.2 PPOç®—æ³•

**ç®—æ³•ç‰¹ç‚¹**:
- On-policy
- è£å‰ªç›®æ ‡å‡½æ•°
- å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡(GAE)
- ç¨³å®šå¯é 

**æ ¸å¿ƒå…¬å¼**:

**è£å‰ªç›®æ ‡**:
```
L(Î¸) = E[min(r(Î¸)A, clip(r(Î¸), 1-Îµ, 1+Îµ)A)]

r(Î¸) = Ï€_Î¸(a|s) / Ï€_old(a|s)  # é‡è¦æ€§æƒé‡
```

**ä¼˜åŠ¿å‡½æ•° (GAE)**:
```
A_t = Î´_t + (Î³Î»)Î´_{t+1} + (Î³Î»)Â²Î´_{t+2} + ...

Î´_t = r_t + Î³V(s_{t+1}) - V(s_t)
```

**RLlibé…ç½®**:
```python
from ray.rllib.algorithms.ppo import PPOConfig

config = PPOConfig()

config.training(
    # Batché…ç½®
    train_batch_size=4000,
    sgd_minibatch_size=128,
    num_sgd_iter=10,

    # PPOå‚æ•°
    lr=3e-4,
    gamma=0.99,
    lambda_=0.95,
    clip_param=0.2,
    vf_clip_param=10.0,
    entropy_coeff=0.01,

    # GAE
    use_gae=True,
    use_critic=True,
)
```

### 4.3 ç®—æ³•å¯¹æ¯”

| ç‰¹æ€§ | SAC | PPO | æ¨èåœºæ™¯ |
|------|-----|-----|---------|
| **æ ·æœ¬æ•ˆç‡** | é«˜ | ä¸­ | SACé€‚åˆæ ·æœ¬æ˜‚è´µ |
| **è®­ç»ƒç¨³å®šæ€§** | ä¸­ | é«˜ | PPOé€‚åˆbaseline |
| **è¶…å‚æ•°æ•æ„Ÿ** | ä½ | ä¸­ | SACæ›´é²æ£’ |
| **è¿ç»­åŠ¨ä½œ** | â­â­â­â­â­ | â­â­â­â­ | SACæœ€ä¼˜ |
| **è®¡ç®—å¼€é”€** | ä¸­ | ä½ | PPOæ›´å¿« |

---

## ç¬¬äº”éƒ¨åˆ†:æ”¹è¿›ä¸ä¼˜åŒ–

### 5.1 è®ºæ–‡è¯„å®¡æ”¹è¿›

**æ”¹è¿›1: ç§»é™¤çŠ¶æ€å™ªå£°**
```python
# åŸç‰ˆ (é”™è¯¯): 17ç»´è§‚æµ‹ + 1ç»´å™ªå£° = 18ç»´
obs = [..., noise]

# æ”¹è¿›: ä»…17ç»´è§‚æµ‹,å™ªå£°ä»…åœ¨åŠ¨ä½œå±‚
obs = [...]  # 17ç»´
action = policy(obs) + exploration_noise  # å™ªå£°åœ¨è¿™é‡Œ
```

**æ”¹è¿›2: åˆ†æ•£é æ³Šå¥–åŠ±**
```python
# åŸç‰ˆ: åŸºäºåˆ°å²¸çº¿ä¸­å¿ƒçš„è·ç¦»
distance_to_center = abs(position - berth_length/2)
reward = -distance_to_center

# æ”¹è¿›: åŸºäºé‚»è¿‘æ‹¥æŒ¤åº¦
neighbor_density = sum(1/(1+dist) for dist in neighbor_distances)
reward = max(0, 2.0 - neighbor_density)
```

**æ”¹è¿›3: èˆ¹èˆ¶ç”Ÿæˆç°å®åŒ–**
```python
# åŸç‰ˆ: å‡åŒ€åˆ†å¸ƒ
vessels = [create_vessel(t) for t in uniform(0, horizon)]

# æ”¹è¿›: éé½æ¬¡æ³Šæ¾ + å¤šå³°åˆ†å¸ƒ
vessels = generate_realistic_vessels(horizon)
```

### 5.2 å¥–åŠ±æƒé‡æ•æ„Ÿæ€§åˆ†æ

```python
# æƒé‡æ‰«æå®éªŒ
weight_configs = [
    {'c1': 1.0, 'c2': 0.3, 'c3': 0.3, 'c4': 0.4, 'c5': 0.6, 'c6': 0.2},
    {'c1': 1.0, 'c2': 0.5, 'c3': 0.3, 'c4': 0.4, 'c5': 0.6, 'c6': 0.2},
    {'c1': 1.0, 'c2': 0.7, 'c3': 0.3, 'c4': 0.4, 'c5': 0.6, 'c6': 0.2},
    # ... æ›´å¤šé…ç½®
]

results = []
for weights in weight_configs:
    reward_calc = RewardCalculator(weights)
    performance = train_and_evaluate(reward_calc)
    results.append({'weights': weights, 'performance': performance})

# åˆ†ææœ€ä¼˜æƒé‡
best_config = max(results, key=lambda x: x['performance'])
```

### 5.3 ç½‘ç»œæ¶æ„ä¼˜åŒ–

```python
# æ ‡å‡†æ¶æ„
config.model = {
    "fcnet_hiddens": [256, 256],
    "fcnet_activation": "relu",
}

# æ·±å±‚æ¶æ„ (æ›´å¼ºè¡¨è¾¾èƒ½åŠ›)
config.model = {
    "fcnet_hiddens": [512, 512, 256],
    "fcnet_activation": "relu",
    "vf_share_layers": False,  # ç‹¬ç«‹ä»·å€¼ç½‘ç»œ
}

# è‡ªå®šä¹‰æ¶æ„
from ray.rllib.models.torch.torch_modelv2 import TorchModelV2

class CustomNetwork(TorchModelV2):
    def __init__(self, obs_space, action_space, num_outputs, model_config, name):
        super().__init__(obs_space, action_space, num_outputs, model_config, name)

        # ç‰¹å¾æå–
        self.feature_net = nn.Sequential(
            nn.Linear(17, 256),
            nn.ReLU(),
            nn.LayerNorm(256),
            nn.Linear(256, 256),
            nn.ReLU(),
        )

        # ç­–ç•¥å¤´
        self.policy_head = nn.Linear(256, num_outputs)

        # ä»·å€¼å¤´
        self.value_head = nn.Linear(256, 1)
```

### 5.4 è¯¾ç¨‹å­¦ä¹ 

```python
# ä»ç®€å•åˆ°å¤æ‚çš„è¯¾ç¨‹
curriculum = [
    {'num_vessels': 5, 'iterations': 500},
    {'num_vessels': 10, 'iterations': 500},
    {'num_vessels': 20, 'iterations': 500},
    {'num_vessels': 50, 'iterations': 1000},
    {'num_vessels': 100, 'iterations': 2000},
]

for stage in curriculum:
    env_config.update({'num_vessels': stage['num_vessels']})
    train(iterations=stage['iterations'])
```

---

## é™„å½•Aï¼šè¯„ä¼°æŒ‡æ ‡

### A.1 æ€§èƒ½æŒ‡æ ‡

```python
def evaluate_performance(env, algo, num_episodes=100):
    """è¯„ä¼°ç®—æ³•æ€§èƒ½"""
    metrics = {
        'total_rewards': [],
        'waiting_times': [],
        'berth_utilization': [],
        'emissions': [],
        'shore_power_usage': [],
        'allocation_success_rate': [],
    }

    for _ in range(num_episodes):
        obs, info = env.reset()
        done = False
        episode_reward = 0

        while not done:
            actions = {
                agent_id: algo.compute_single_action(obs[agent_id])
                for agent_id in obs.keys()
            }
            obs, rewards, dones, truncs, infos = env.step(actions)
            episode_reward += sum(rewards.values())
            done = dones["__all__"]

        # æ”¶é›†æŒ‡æ ‡
        metrics['total_rewards'].append(episode_reward)
        metrics['waiting_times'].append(env.avg_waiting_time)
        metrics['berth_utilization'].append(env.berth_utilization)
        metrics['emissions'].append(env.total_emissions)
        metrics['shore_power_usage'].append(env.shore_power_usage_rate)
        metrics['allocation_success_rate'].append(env.success_rate)

    # ç»Ÿè®¡
    return {
        key: {
            'mean': np.mean(values),
            'std': np.std(values),
            'min': np.min(values),
            'max': np.max(values),
        }
        for key, values in metrics.items()
    }
```

### A.2 å¯¹æ¯”åŸºçº¿

```python
# GreedyåŸºçº¿
def greedy_baseline(env):
    """è´ªå¿ƒç®—æ³•: FCFS + æœ€æ—©å¯ç”¨ä½ç½®"""
    vessels = sorted(env.vessels, key=lambda v: v.arrival_time)

    for vessel in vessels:
        # æ‰¾æœ€æ—©å¯ç”¨ä½ç½®
        position = find_earliest_available_position(vessel)
        env.allocate(vessel, position, waiting_time=0, shore_power_prob=1.0)

# FCFSåŸºçº¿
def fcfs_baseline(env):
    """å…ˆåˆ°å…ˆæœåŠ¡"""
    vessels = sorted(env.vessels, key=lambda v: v.arrival_time)

    for vessel in vessels:
        position = 0  # æ€»æ˜¯ä»0å¼€å§‹
        env.allocate(vessel, position, waiting_time=0, shore_power_prob=0)
```

---

## é™„å½•Bï¼šå®éªŒè®¾ç½®

### B.1 é»˜è®¤å‚æ•°

```python
default_config = {
    # ç¯å¢ƒ
    'num_vessels': 50,
    'planning_horizon_days': 7,
    'berth_length': 2000.0,
    'max_waiting_time': 24.0,

    # å¥–åŠ±æƒé‡
    'c1': 1.0,
    'c2': 0.5,
    'c3': 0.3,
    'c4': 0.4,
    'c5': 0.6,
    'c6': 0.2,

    # è®­ç»ƒ
    'num_iterations': 1000,
    'batch_size': 256,
    'learning_rate': 3e-4,
}
```

### B.2 å®éªŒåœºæ™¯

| åœºæ™¯ | èˆ¹èˆ¶æ•° | ç›®æ ‡ |
|------|--------|------|
| å°è§„æ¨¡ | 10-20 | ç®—æ³•éªŒè¯ |
| ä¸­è§„æ¨¡ | 50 | æ€§èƒ½å¯¹æ¯” |
| å¤§è§„æ¨¡ | 100 | å¯æ‰©å±•æ€§ |
| è¶…å¤§è§„æ¨¡ | 200+ | æé™æµ‹è¯• |

---

**æ–‡æ¡£ç»´æŠ¤**: Duan
**æœ€åæ›´æ–°**: 2025-10-28
**ç‰ˆæœ¬**: v2.0
