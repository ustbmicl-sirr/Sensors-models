# RLlibå®æ–½æ–¹æ¡ˆ - æ³Šä½åˆ†é…MARLé¡¹ç›®

**æ¡†æ¶**: Ray RLlib
**ç¯å¢ƒ**: Macæœ¬åœ°å¼€å‘ + äº‘GPUè®­ç»ƒ
**æ—¶é—´**: 3ä¸ªæœˆï¼ˆ2025å¹´10æœˆ - 2026å¹´1æœˆï¼‰
**ç®—æ³•**: å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMADDPG, MAPPOç­‰ï¼‰

---

## ğŸ¯ é¡¹ç›®ç›®æ ‡

1. âœ… ä½¿ç”¨RLlibå®ç°æ³Šä½åˆ†é…çš„å¤šæ™ºèƒ½ä½“ç¯å¢ƒ
2. âœ… è®­ç»ƒMADDPGç­‰MARLç®—æ³•
3. âœ… åœ¨äº‘GPUä¸Šè¿›è¡Œå¤§è§„æ¨¡å®éªŒ
4. âœ… å®Œæˆå­¦æœ¯è®ºæ–‡ï¼ˆ2026å¹´1æœˆå‰ï¼‰

---

## ğŸ“… æ—¶é—´è§„åˆ’ï¼ˆ12å‘¨ï¼‰

```
Week 1 (10æœˆæœ«):        RLlibç¯å¢ƒå¼€å‘ï¼ˆMacæœ¬åœ°ï¼‰
Week 2 (11æœˆåˆ):        å¤šæ™ºèƒ½ä½“ç®—æ³•é…ç½®
Week 3 (11æœˆä¸­):        äº‘å¹³å°éƒ¨ç½²å‡†å¤‡
Week 4-6 (11æœˆ):        äº‘ç«¯å¤§è§„æ¨¡å®éªŒ
Week 7-10 (12æœˆ):       è®ºæ–‡æ’°å†™
Week 11-12 (1æœˆåˆ):     ä¿®æ”¹æ¶¦è‰²æŠ•ç¨¿
```

---

## ğŸ—ï¸ é˜¶æ®µ1: RLlibç¯å¢ƒå¼€å‘ï¼ˆWeek 1ï¼‰

### 1.1 å®‰è£…ä¾èµ–ï¼ˆDay 1ï¼‰

#### Macæœ¬åœ°å¼€å‘ç¯å¢ƒ

```bash
# 1. å®‰è£…Rayå’ŒRLlib
conda activate marl-task
pip install "ray[rllib]">=2.9.0
pip install "gymnasium>=0.28.0"
pip install torch  # PyTorchæ¡†æ¶

# 2. éªŒè¯å®‰è£…
python -c "import ray; ray.init(); print('Ray version:', ray.__version__)"
python -c "from ray.rllib.algorithms.ppo import PPOConfig; print('RLlib OK')"

# 3. å®‰è£…å¯é€‰ä¾èµ–
pip install tensorboard
pip install wandb  # Weights & Biasesæ—¥å¿—ï¼ˆå¯é€‰ï¼‰
```

#### äº‘å¹³å°ç¯å¢ƒï¼ˆåç»­å‡†å¤‡ï¼‰

```bash
# å°†åœ¨Week 3å‡†å¤‡
# æ”¯æŒçš„äº‘å¹³å°ï¼š
# - AWS (æ¨è): p3.2xlarge (V100 GPU)
# - Google Cloud: n1-standard-8 + T4 GPU
# - é˜¿é‡Œäº‘: ecs.gn6i-c4g1.xlarge (T4 GPU)
```

---

### 1.2 åˆ›å»ºRLlibå¤šæ™ºèƒ½ä½“ç¯å¢ƒï¼ˆDay 2-4ï¼‰

#### æ–‡ä»¶ç»“æ„

```
MARL-task/
â”œâ”€â”€ rllib_env/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ berth_allocation_env.py    # æ ¸å¿ƒç¯å¢ƒ
â”‚   â”œâ”€â”€ utils.py                    # å·¥å…·å‡½æ•°
â”‚   â””â”€â”€ test_env.py                 # æµ‹è¯•è„šæœ¬
â”œâ”€â”€ environment/                     # å¤ç”¨ç°æœ‰ä»£ç 
â”‚   â”œâ”€â”€ vessel.py
â”‚   â”œâ”€â”€ shore_power.py
â”‚   â””â”€â”€ ...
â””â”€â”€ rllib_train.py                  # è®­ç»ƒè„šæœ¬
```

#### æ ¸å¿ƒä»£ç ï¼š`rllib_env/berth_allocation_env.py`

```python
"""
RLlibå¤šæ™ºèƒ½ä½“æ³Šä½åˆ†é…ç¯å¢ƒ
"""
from typing import Dict, Optional
import gymnasium as gym
from gymnasium.spaces import Box, Dict as DictSpace
import numpy as np
from ray.rllib.env.multi_agent_env import MultiAgentEnv

# å¯¼å…¥ç°æœ‰ç»„ä»¶
import sys
sys.path.append('..')
from environment.vessel import Vessel, VesselGenerator
from environment.shore_power import ShorePowerManager
from rewards.reward_calculator import RewardCalculator


class BerthAllocationMultiAgentEnv(MultiAgentEnv):
    """
    æ³Šä½åˆ†é…å¤šæ™ºèƒ½ä½“ç¯å¢ƒ - RLlibé€‚é…

    ç‰¹æ€§:
    - æ¯è‰˜èˆ¹æ˜¯ä¸€ä¸ªæ™ºèƒ½ä½“
    - è¿ç»­åŠ¨ä½œç©ºé—´ (3ç»´)
    - éƒ¨åˆ†å¯è§‚æµ‹ (POMDP)
    - ä¸ªä½“å¥–åŠ± (æ¯è‰˜èˆ¹ç‹¬ç«‹å¥–åŠ±)
    """

    def __init__(self, config: dict):
        super().__init__()

        # ç¯å¢ƒå‚æ•°
        self.berth_length = config.get('berth_length', 2000)
        self.max_vessels = config.get('max_vessels', 20)
        self.planning_horizon = config.get('planning_horizon', 168)  # 7å¤©

        # èˆ¹èˆ¶ç”Ÿæˆå™¨
        self.vessel_generator = VesselGenerator(config)

        # å²¸ç”µç®¡ç†å™¨
        if config.get('shore_power_enabled', True):
            self.shore_power_manager = ShorePowerManager(config)
        else:
            self.shore_power_manager = None

        # å¥–åŠ±è®¡ç®—å™¨
        self.reward_calculator = RewardCalculator(config)

        # ç¯å¢ƒçŠ¶æ€
        self.vessels = []
        self.allocations = []
        self.current_step = 0
        self.current_time = 0

        # å®šä¹‰è§‚æµ‹ç©ºé—´ (17ç»´å±€éƒ¨è§‚æµ‹)
        self._obs_space = Box(
            low=-1.0,
            high=1.0,
            shape=(17,),
            dtype=np.float32
        )

        # å®šä¹‰åŠ¨ä½œç©ºé—´ (3ç»´è¿ç»­åŠ¨ä½œ)
        self._action_space = Box(
            low=-1.0,
            high=1.0,
            shape=(3,),
            dtype=np.float32
        )

        # RLlibè¦æ±‚çš„æ™ºèƒ½ä½“IDé›†åˆ
        self._agent_ids = set()

    @property
    def observation_space(self):
        """è¿”å›è§‚æµ‹ç©ºé—´"""
        return self._obs_space

    @property
    def action_space(self):
        """è¿”å›åŠ¨ä½œç©ºé—´"""
        return self._action_space

    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None):
        """
        é‡ç½®ç¯å¢ƒ

        Returns:
            observations: Dict[agent_id, obs]
            infos: Dict[agent_id, info]
        """
        # è®¾ç½®éšæœºç§å­
        if seed is not None:
            np.random.seed(seed)

        # ç”Ÿæˆèˆ¹èˆ¶
        if options and 'vessels' in options:
            self.vessels = options['vessels']
        else:
            self.vessels = self.vessel_generator.generate_vessels(
                num_vessels=self.max_vessels
            )

        # é‡ç½®çŠ¶æ€
        self.allocations = []
        self.current_step = 0
        self.current_time = 0

        # è®¾ç½®æ™ºèƒ½ä½“IDï¼ˆæ¯è‰˜èˆ¹ä¸€ä¸ªIDï¼‰
        self._agent_ids = {f"vessel_{i}" for i in range(len(self.vessels))}

        # è·å–åˆå§‹è§‚æµ‹
        observations = self._get_observations()
        infos = {agent_id: {} for agent_id in self._agent_ids}

        return observations, infos

    def step(self, action_dict: Dict[str, np.ndarray]):
        """
        æ‰§è¡ŒåŠ¨ä½œ

        Args:
            action_dict: Dict[agent_id, action]
                action: [position, wait_time, shore_power_prob]
                       èŒƒå›´: [-1, 1]

        Returns:
            observations: Dict[agent_id, obs]
            rewards: Dict[agent_id, float]
            terminateds: Dict[agent_id, bool]
            truncateds: Dict[agent_id, bool]
            infos: Dict[agent_id, dict]
        """
        observations = {}
        rewards = {}
        terminateds = {}
        truncateds = {}
        infos = {}

        # å¤„ç†æ¯ä¸ªæ™ºèƒ½ä½“çš„åŠ¨ä½œ
        for agent_id, action in action_dict.items():
            vessel_idx = int(agent_id.split('_')[1])
            vessel = self.vessels[vessel_idx]

            # è§£ç åŠ¨ä½œ (ä»[-1,1]æ˜ å°„åˆ°å®é™…èŒƒå›´)
            position = self._decode_position(action[0])
            wait_time = self._decode_wait_time(action[1])
            shore_power_prob = self._decode_shore_power(action[2])

            # å†³å®šæ˜¯å¦ä½¿ç”¨å²¸ç”µ
            uses_shore_power = (
                vessel.can_use_shore_power and
                np.random.random() < shore_power_prob
            )

            # åˆ›å»ºåˆ†é…
            allocation = {
                'vessel': vessel,
                'vessel_id': vessel.id,
                'position': position,
                'berthing_time': vessel.arrival_time + wait_time,
                'departure_time': vessel.arrival_time + wait_time + vessel.operation_time,
                'uses_shore_power': uses_shore_power,
                'waiting_time': wait_time,
            }

            # æ·»åŠ åˆ°åˆ†é…åˆ—è¡¨
            self.allocations.append(allocation)

            # è®¡ç®—å¥–åŠ±
            reward = self.reward_calculator.calculate_reward(
                allocation,
                self.allocations,
                self.shore_power_manager
            )

            # è·å–è§‚æµ‹
            obs = self._get_observation(vessel_idx)

            # å¡«å……è¿”å›å€¼
            observations[agent_id] = obs
            rewards[agent_id] = reward
            terminateds[agent_id] = False  # åˆ†æ‰¹å†³ç­–ï¼Œä¸æå‰ç»ˆæ­¢
            truncateds[agent_id] = False
            infos[agent_id] = {
                'allocated': True,
                'position': position,
                'berthing_time': allocation['berthing_time'],
                'uses_shore_power': uses_shore_power
            }

        # æ›´æ–°æ—¶é—´æ­¥
        self.current_step += 1

        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰èˆ¹èˆ¶éƒ½å·²åˆ†é…
        if len(self.allocations) >= len(self.vessels):
            # Episodeç»“æŸ
            terminateds['__all__'] = True
            truncateds['__all__'] = False
        else:
            terminateds['__all__'] = False
            truncateds['__all__'] = False

        return observations, rewards, terminateds, truncateds, infos

    def _get_observations(self) -> Dict[str, np.ndarray]:
        """è·å–æ‰€æœ‰æ™ºèƒ½ä½“çš„è§‚æµ‹"""
        observations = {}
        for i, vessel in enumerate(self.vessels):
            agent_id = f"vessel_{i}"
            observations[agent_id] = self._get_observation(i)
        return observations

    def _get_observation(self, vessel_idx: int) -> np.ndarray:
        """
        è·å–å•ä¸ªæ™ºèƒ½ä½“çš„è§‚æµ‹ (17ç»´)

        è§‚æµ‹å†…å®¹:
        - é™æ€ç‰¹å¾ (4): èˆ¹é•¿, åˆ°æ¸¯æ—¶é—´, ä¼˜å…ˆçº§, å²¸ç”µèƒ½åŠ›
        - åŠ¨æ€ç‰¹å¾ (3): å½“å‰æ—¶é—´, ç­‰å¾…æ—¶é—´, æ“ä½œæ—¶é—´
        - å²¸ç”µä¿¡æ¯ (6): 5æ®µä½¿ç”¨ç‡ + æ€»ä½¿ç”¨ç‡
        - æ³Šä½ä¿¡æ¯ (4): å·¦å³é‚»è¿‘è·ç¦», å¯ç”¨ç©ºé—´, å ç”¨ç‡
        """
        vessel = self.vessels[vessel_idx]

        # é™æ€ç‰¹å¾ï¼ˆå½’ä¸€åŒ–ï¼‰
        vessel_length = vessel.length / self.berth_length
        arrival_time = vessel.arrival_time / self.planning_horizon
        priority = vessel.priority / 4.0
        shore_power_cap = 1.0 if vessel.can_use_shore_power else 0.0

        # åŠ¨æ€ç‰¹å¾
        current_time = self.current_time / self.planning_horizon
        waiting_time = max(0, self.current_time - vessel.arrival_time) / 48.0
        operation_time = vessel.operation_time / self.planning_horizon

        # å²¸ç”µä½¿ç”¨ç‡ï¼ˆå¦‚æœæœ‰ï¼‰
        if self.shore_power_manager:
            shore_power_usage = self.shore_power_manager.get_usage_rates()
        else:
            shore_power_usage = [0.0] * 6

        # æ³Šä½ä¿¡æ¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
        left_distance = 1.0  # é»˜è®¤å€¼
        right_distance = 1.0
        available_space = 1.0
        berth_occupancy = len(self.allocations) / self.max_vessels

        # ç»„åˆè§‚æµ‹
        obs = np.array([
            vessel_length,
            arrival_time,
            priority,
            shore_power_cap,
            current_time,
            waiting_time,
            operation_time,
            *shore_power_usage,
            left_distance,
            right_distance,
            available_space,
            berth_occupancy,
        ], dtype=np.float32)

        return obs

    def _decode_position(self, action_value: float) -> float:
        """è§£ç ä½ç½®åŠ¨ä½œ"""
        # action_value âˆˆ [-1, 1]
        # position âˆˆ [0, berth_length]
        position = (action_value + 1) * self.berth_length / 2
        return np.clip(position, 0, self.berth_length)

    def _decode_wait_time(self, action_value: float) -> float:
        """è§£ç ç­‰å¾…æ—¶é—´åŠ¨ä½œ"""
        # action_value âˆˆ [-1, 1]
        # wait_time âˆˆ [0, 48]å°æ—¶
        wait_time = (action_value + 1) * 24  # 0-48å°æ—¶
        return np.clip(wait_time, 0, 48)

    def _decode_shore_power(self, action_value: float) -> float:
        """è§£ç å²¸ç”µæ¦‚ç‡"""
        # action_value âˆˆ [-1, 1]
        # probability âˆˆ [0, 1]
        prob = (action_value + 1) / 2
        return np.clip(prob, 0, 1)
```

---

### 1.3 æµ‹è¯•ç¯å¢ƒï¼ˆDay 4ï¼‰

#### æµ‹è¯•è„šæœ¬ï¼š`rllib_env/test_env.py`

```python
"""æµ‹è¯•RLlibç¯å¢ƒ"""
import numpy as np
from berth_allocation_env import BerthAllocationMultiAgentEnv

# é…ç½®
config = {
    'berth_length': 2000,
    'max_vessels': 5,  # æµ‹è¯•ç”¨å°æ•°é‡
    'planning_horizon': 168,
    'shore_power_enabled': True,
    'generation_mode': 'simple',
}

# åˆ›å»ºç¯å¢ƒ
env = BerthAllocationMultiAgentEnv(config)

# é‡ç½®
obs, info = env.reset()
print(f"åˆå§‹è§‚æµ‹: {len(obs)} ä¸ªæ™ºèƒ½ä½“")
print(f"è§‚æµ‹ç©ºé—´: {env.observation_space}")
print(f"åŠ¨ä½œç©ºé—´: {env.action_space}")

# è¿è¡Œå‡ æ­¥
for step in range(5):
    # éšæœºåŠ¨ä½œ
    actions = {
        agent_id: env.action_space.sample()
        for agent_id in obs.keys()
    }

    obs, rewards, dones, truncs, infos = env.step(actions)

    print(f"\næ­¥éª¤ {step+1}:")
    print(f"  å¥–åŠ±: {rewards}")
    print(f"  å®Œæˆ: {dones}")

    if dones.get('__all__', False):
        print("Episode å®Œæˆ!")
        break

print("\nâœ… ç¯å¢ƒæµ‹è¯•é€šè¿‡!")
```

è¿è¡Œæµ‹è¯•:

```bash
cd rllib_env
python test_env.py
```

---

## ğŸ—ï¸ é˜¶æ®µ2: å¤šæ™ºèƒ½ä½“ç®—æ³•é…ç½®ï¼ˆWeek 2ï¼‰

### 2.1 é…ç½®MADDPGç®—æ³•ï¼ˆDay 1-3ï¼‰

#### è®­ç»ƒè„šæœ¬ï¼š`rllib_train.py`

```python
"""
RLlibè®­ç»ƒè„šæœ¬ - MADDPGç®—æ³•
"""
import ray
from ray import tune
from ray.rllib.algorithms.maddpg import MADDPGConfig
from ray.rllib.policy.policy import PolicySpec
import os

# å¯¼å…¥ç¯å¢ƒ
from rllib_env.berth_allocation_env import BerthAllocationMultiAgentEnv

# åˆå§‹åŒ–Ray
ray.init(num_gpus=1)  # äº‘ç«¯è®­ç»ƒæ—¶ä½¿ç”¨GPU

# ç¯å¢ƒé…ç½®
env_config = {
    'berth_length': 2000,
    'max_vessels': 20,
    'planning_horizon': 168,
    'shore_power_enabled': True,
    'generation_mode': 'realistic',
    'reward_weights': {
        'c1_base': 100.0,
        'c2_waiting': 10.0,
        'c3_emission': 0.01,
        'c4_shore_power': 50.0,
        'c5_utilization': 200.0,
        'c6_spacing': 30.0,
    }
}

# åˆ›å»ºä¸´æ—¶ç¯å¢ƒè·å–æ™ºèƒ½ä½“æ•°é‡
temp_env = BerthAllocationMultiAgentEnv(env_config)
temp_env.reset()
num_agents = len(temp_env._agent_ids)
temp_env.close()

# å®šä¹‰ç­–ç•¥ï¼ˆæ¯ä¸ªæ™ºèƒ½ä½“å¯ä»¥æœ‰ç‹¬ç«‹ç­–ç•¥ï¼‰
policies = {
    f"policy_vessel_{i}": PolicySpec(
        policy_class=None,  # ä½¿ç”¨é»˜è®¤ç­–ç•¥
        observation_space=temp_env.observation_space,
        action_space=temp_env.action_space,
        config={},
    )
    for i in range(num_agents)
}

# ç­–ç•¥æ˜ å°„å‡½æ•°
def policy_mapping_fn(agent_id, episode, worker, **kwargs):
    """å°†æ™ºèƒ½ä½“IDæ˜ å°„åˆ°ç­–ç•¥"""
    # ç®€å•ç­–ç•¥ï¼šæ¯ä¸ªvesselä½¿ç”¨å¯¹åº”çš„policy
    vessel_idx = int(agent_id.split('_')[1])
    return f"policy_vessel_{vessel_idx}"

# é…ç½®MADDPG
config = (
    MADDPGConfig()
    .environment(
        env=BerthAllocationMultiAgentEnv,
        env_config=env_config,
    )
    .framework("torch")
    .training(
        actor_lr=1e-4,
        critic_lr=1e-3,
        tau=0.005,
        gamma=0.99,
        n_step=1,
        replay_buffer_config={
            "type": "MultiAgentReplayBuffer",
            "capacity": 1000000,
        },
        train_batch_size=256,
    )
    .multi_agent(
        policies=policies,
        policy_mapping_fn=policy_mapping_fn,
    )
    .resources(
        num_gpus=1,          # ä½¿ç”¨1ä¸ªGPU
        num_cpus_per_worker=1,
    )
    .rollouts(
        num_rollout_workers=4,  # 4ä¸ªå¹¶è¡Œé‡‡æ ·å™¨
        num_envs_per_worker=1,
    )
    .reporting(
        min_train_timesteps_per_iteration=1000,
        min_sample_timesteps_per_iteration=1000,
    )
)

# è®­ç»ƒ
results = tune.run(
    "MADDPG",
    config=config.to_dict(),
    stop={
        "timesteps_total": 1000000,  # 1M steps
    },
    checkpoint_freq=10,
    checkpoint_at_end=True,
    local_dir="./ray_results",
    verbose=1,
)

print("è®­ç»ƒå®Œæˆ!")
print(f"æœ€ä½³æ£€æŸ¥ç‚¹: {results.best_checkpoint}")
```

---

### 2.2 é…ç½®å…¶ä»–ç®—æ³•ï¼ˆDay 4-5ï¼‰

#### MAPPOé…ç½®

```python
from ray.rllib.algorithms.ppo import PPOConfig

config = (
    PPOConfig()
    .environment(env=BerthAllocationMultiAgentEnv, env_config=env_config)
    .framework("torch")
    .training(
        lr=5e-5,
        gamma=0.99,
        lambda_=0.95,
        clip_param=0.2,
        vf_clip_param=10.0,
        entropy_coeff=0.01,
    )
    .multi_agent(
        policies=policies,
        policy_mapping_fn=policy_mapping_fn,
    )
    .resources(num_gpus=1, num_cpus_per_worker=1)
    .rollouts(num_rollout_workers=4, num_envs_per_worker=1)
)
```

#### QMIXé…ç½®ï¼ˆå¦‚éœ€è¦ï¼‰

```python
from ray.rllib.algorithms.qmix import QMixConfig

# æ³¨æ„ï¼šQMIXéœ€è¦ç¦»æ•£åŠ¨ä½œï¼Œéœ€è¦ä¿®æ”¹ç¯å¢ƒ
# æˆ–ä½¿ç”¨ç¦»æ•£åŒ–ç‰ˆæœ¬çš„æ³Šä½åˆ†é…ç¯å¢ƒ
```

---

## ğŸ—ï¸ é˜¶æ®µ3: äº‘å¹³å°éƒ¨ç½²ï¼ˆWeek 3ï¼‰

### 3.1 å‡†å¤‡Dockeré•œåƒï¼ˆæ¨èï¼‰

#### Dockerfile

```dockerfile
FROM rayproject/ray:latest-gpu

# å®‰è£…ä¾èµ–
COPY requirements.txt /workspace/
RUN pip install -r /workspace/requirements.txt

# å¤åˆ¶ä»£ç 
COPY . /workspace/
WORKDIR /workspace

# å¯åŠ¨å‘½ä»¤
CMD ["python", "rllib_train.py"]
```

### 3.2 äº‘å¹³å°é€‰æ‹©

#### æ¨èæ–¹æ¡ˆ1: AWS EC2

```bash
# å®ä¾‹ç±»å‹: p3.2xlarge (V100 GPU, 8 vCPUs, 61GB RAM)
# ä»·æ ¼: ~$3/å°æ—¶

# å¯åŠ¨å®ä¾‹
aws ec2 run-instances \
  --image-id ami-xxxxx \
  --instance-type p3.2xlarge \
  --key-name your-key \
  --security-group-ids sg-xxxxx

# SSHè¿æ¥
ssh -i your-key.pem ubuntu@ec2-xxx.compute.amazonaws.com

# å®‰è£…ç¯å¢ƒ
git clone your-repo
cd MARL-task
pip install -r requirements.txt

# è¿è¡Œè®­ç»ƒ
python rllib_train.py
```

#### æ¨èæ–¹æ¡ˆ2: Google Colab Proï¼ˆæœ€ç®€å•ï¼‰

```python
# Colabç¬”è®°æœ¬
!git clone https://github.com/your-username/MARL-task.git
%cd MARL-task

!pip install ray[rllib] torch

# è¿è¡Œè®­ç»ƒ
!python rllib_train.py
```

#### æ¨èæ–¹æ¡ˆ3: é˜¿é‡Œäº‘ECS

```bash
# å®ä¾‹: ecs.gn6i-c4g1.xlarge (T4 GPU)
# ä»·æ ¼: ~Â¥8/å°æ—¶

# é•œåƒ: Ubuntu 20.04 + CUDA 11.8
```

---

## ğŸ—ï¸ é˜¶æ®µ4: å¤§è§„æ¨¡å®éªŒï¼ˆWeek 4-6ï¼‰

### 4.1 å®éªŒè®¾è®¡

#### å®éªŒçŸ©é˜µ

```python
# experiments/experiment_configs.py

SCENARIOS = {
    'small': {'max_vessels': 10, 'planning_horizon': 168},
    'medium': {'max_vessels': 15, 'planning_horizon': 168},
    'large': {'max_vessels': 20, 'planning_horizon': 168},
    'xlarge': {'max_vessels': 25, 'planning_horizon': 168},
}

ALGORITHMS = ['MADDPG', 'MAPPO', 'IPPO']

CONFIGURATIONS = {
    'shore_power': [True, False],
    'generation_mode': ['realistic', 'simple'],
}

SEEDS = [42, 123, 456, 789, 1024]

# æ€»å®éªŒæ•°
total_experiments = (
    len(SCENARIOS) *
    len(ALGORITHMS) *
    len(CONFIGURATIONS['shore_power']) *
    len(CONFIGURATIONS['generation_mode']) *
    len(SEEDS)
)
# = 4 * 3 * 2 * 2 * 5 = 240ç»„å®éªŒ
```

### 4.2 æ‰¹é‡è®­ç»ƒè„šæœ¬

```python
# experiments/run_all_experiments.py

import itertools
import subprocess
import json

# ç”Ÿæˆæ‰€æœ‰å®éªŒç»„åˆ
experiments = []
for scenario, algo, shore_power, gen_mode, seed in itertools.product(
    SCENARIOS.keys(),
    ALGORITHMS,
    CONFIGURATIONS['shore_power'],
    CONFIGURATIONS['generation_mode'],
    SEEDS
):
    exp_config = {
        'scenario': scenario,
        'algorithm': algo,
        'shore_power': shore_power,
        'generation_mode': gen_mode,
        'seed': seed,
        'name': f"{algo}_{scenario}_sp{shore_power}_gm{gen_mode}_s{seed}"
    }
    experiments.append(exp_config)

print(f"æ€»å…± {len(experiments)} ä¸ªå®éªŒ")

# è¿è¡Œæ‰€æœ‰å®éªŒ
for i, exp in enumerate(experiments):
    print(f"\n[{i+1}/{len(experiments)}] è¿è¡Œ: {exp['name']}")

    # æ„å»ºå‘½ä»¤
    cmd = [
        "python", "rllib_train.py",
        "--algo", exp['algorithm'],
        "--scenario", exp['scenario'],
        "--shore-power", str(exp['shore_power']),
        "--gen-mode", exp['generation_mode'],
        "--seed", str(exp['seed']),
        "--name", exp['name'],
    ]

    # è¿è¡Œ
    subprocess.run(cmd)

print("\nâœ… æ‰€æœ‰å®éªŒå®Œæˆ!")
```

### 4.3 é¢„è®¡è®­ç»ƒæ—¶é—´

```python
# å•ä¸ªå®éªŒï¼ˆ1M stepsï¼‰:
# - CPU: 4-6å°æ—¶
# - GPU (T4): 30-60åˆ†é’Ÿ
# - GPU (V100): 15-30åˆ†é’Ÿ

# 240ä¸ªå®éªŒ:
# - ä¸²è¡Œ (V100): 240 * 0.5h = 120å°æ—¶ = 5å¤©
# - 4ä¸ªGPUå¹¶è¡Œ: 120h / 4 = 30å°æ—¶ â‰ˆ 1.5å¤©

# é¢„ç®—ä¼°ç®—ï¼ˆAWS p3.2xlarge, $3/å°æ—¶ï¼‰:
# 240å®éªŒ * 0.5h * $3 = $360
# æˆ–ä½¿ç”¨Spotå®ä¾‹: ~$100-150
```

---

## ğŸ“Š é˜¶æ®µ5: ç»“æœåˆ†æï¼ˆWeek 6ï¼‰

### 5.1 æå–è®­ç»ƒæ•°æ®

```python
# analysis/extract_results.py

from ray.tune import Analysis
import pandas as pd

# åŠ è½½æ‰€æœ‰å®éªŒç»“æœ
analysis = Analysis("./ray_results")

# æå–å…³é”®æŒ‡æ ‡
results = []
for trial in analysis.trials:
    config = trial.config
    last_result = trial.last_result

    results.append({
        'algorithm': config['algorithm'],
        'scenario': config['scenario'],
        'shore_power': config['shore_power'],
        'seed': config['seed'],
        'reward_mean': last_result.get('episode_reward_mean'),
        'berth_utilization': last_result.get('custom_metrics/berth_utilization'),
        'avg_waiting_time': last_result.get('custom_metrics/avg_waiting_time'),
        'total_emissions': last_result.get('custom_metrics/total_emissions'),
    })

df = pd.DataFrame(results)
df.to_csv('results_summary.csv', index=False)
print(df.groupby(['algorithm', 'scenario']).mean())
```

### 5.2 ç»˜å›¾

```python
# analysis/plot_results.py

import matplotlib.pyplot as plt
import seaborn as sns

# 1. è®­ç»ƒæ›²çº¿å¯¹æ¯”
for algo in ALGORITHMS:
    algo_data = df[df['algorithm'] == algo]
    plt.plot(algo_data['timesteps'], algo_data['reward'], label=algo)
plt.xlabel('Timesteps')
plt.ylabel('Reward')
plt.legend()
plt.savefig('training_curves.pdf')

# 2. æ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾
sns.barplot(data=df, x='algorithm', y='berth_utilization', hue='scenario')
plt.savefig('performance_comparison.pdf')

# 3. é›·è¾¾å›¾ï¼ˆå¤šæŒ‡æ ‡ï¼‰
# ...

print("å›¾è¡¨å·²ç”Ÿæˆ!")
```

---

## ğŸ“ é˜¶æ®µ6: è®ºæ–‡æ’°å†™ï¼ˆWeek 7-12ï¼‰

ï¼ˆä¸ä¹‹å‰æ–¹æ¡ˆç±»ä¼¼ï¼Œä½†å¼ºè°ƒRLlibçš„ä¼˜åŠ¿ï¼‰

### è®ºæ–‡ç»“æ„

```
I. Introduction
   - å¼ºè°ƒï¼šä½¿ç”¨å·¥ä¸šçº§MARLæ¡†æ¶ï¼ˆRLlibï¼‰

II. Related Work
   - åŒ…æ‹¬ï¼šRay RLlibåœ¨MARLä¸­çš„åº”ç”¨

III. Problem Formulation
   (ä¸ä¹‹å‰ç›¸åŒ)

IV. Methodology
   A. MADDPGç®—æ³•
   B. RLlibå®ç°ç»†èŠ‚
   C. åˆ†å¸ƒå¼è®­ç»ƒç­–ç•¥

V. Experiments
   A. å®éªŒç¯å¢ƒï¼ˆRLlib + GPUé›†ç¾¤ï¼‰
   B. åŸºå‡†å¯¹æ¯”ï¼ˆMADDPG vs MAPPO vs IPPOï¼‰
   C. æ¶ˆèå®éªŒ

VI. Results
   - å¼ºè°ƒï¼šå¤§è§„æ¨¡å®éªŒï¼ˆ240ç»„ï¼‰
   - å¼ºè°ƒï¼šè®­ç»ƒæ•ˆç‡ï¼ˆGPUåŠ é€Ÿï¼‰

VII. Conclusion
```

---

## ğŸ¯ å…³é”®ä¼˜åŠ¿æ€»ç»“

### RLlibå¸¦æ¥çš„ä¼˜åŠ¿

1. **è®­ç»ƒé€Ÿåº¦** âš¡
   - GPUåŠ é€Ÿ: æ¯”CPUå¿«10-20å€
   - åˆ†å¸ƒå¼: 4ä¸ªGPUå¹¶è¡Œï¼Œæ€»æ—¶é—´ç¼©çŸ­75%

2. **å®éªŒè§„æ¨¡** ğŸ“Š
   - 240ç»„å¤§è§„æ¨¡å®éªŒå¯è¡Œ
   - å®Œæ•´çš„æ¶ˆèç ”ç©¶

3. **ä»£ç è´¨é‡** ğŸ’»
   - å·¥ä¸šçº§æ¡†æ¶ï¼Œå°‘Bug
   - è¯¦ç»†æ–‡æ¡£ï¼Œæ˜“è°ƒè¯•

4. **æœªæ¥ä»·å€¼** ğŸš€
   - è®­ç»ƒå¥½çš„æ¨¡å‹å¯ç›´æ¥éƒ¨ç½²
   - è®ºæ–‡+å®é™…åº”ç”¨åŒé‡ä»·å€¼

---

## âœ… ç«‹å³è¡ŒåŠ¨è®¡åˆ’

### æœ¬å‘¨ä»»åŠ¡ï¼ˆWeek 1ï¼‰

**Day 1**:
- [ ] å®‰è£…Rayå’ŒRLlib
- [ ] éªŒè¯ç¯å¢ƒ

**Day 2-3**:
- [ ] å®ç°`BerthAllocationMultiAgentEnv`
- [ ] å¤ç”¨ç°æœ‰environmentä»£ç 

**Day 4**:
- [ ] æµ‹è¯•ç¯å¢ƒ
- [ ] ä¿®å¤Bug

**Day 5**:
- [ ] åˆ›å»ºMADDPGè®­ç»ƒè„šæœ¬
- [ ] æœ¬åœ°å°è§„æ¨¡æµ‹è¯•

**å‘¨æœ«**:
- [ ] å‡†å¤‡äº‘å¹³å°è´¦å·
- [ ] é¢„ç®—è§„åˆ’

---

## ğŸ“‹ éœ€è¦ç¡®è®¤

1. **äº‘å¹³å°é€‰æ‹©**:
   - [ ] AWSï¼ˆæ¨èï¼Œçµæ´»ï¼‰
   - [ ] Google Colab Proï¼ˆæœ€ç®€å•ï¼Œ$10/æœˆï¼‰
   - [ ] é˜¿é‡Œäº‘ï¼ˆå›½å†…å¿«ï¼‰

2. **é¢„ç®—**:
   - å®éªŒæˆæœ¬: $100-400
   - èƒ½å¦æ¥å—ï¼Ÿ

3. **ç«‹å³å¼€å§‹**ï¼Ÿ
   - æˆ‘å¯ä»¥ç°åœ¨å°±å¸®æ‚¨åˆ›å»ºç¬¬ä¸€ä¸ªRLlibç¯å¢ƒæ–‡ä»¶ï¼

**è¯·å‘Šè¯‰æˆ‘æ‚¨çš„å†³å®šï¼Œæˆ‘ä»¬ç«‹å³å¼€å§‹å®æ–½ï¼** ğŸš€
