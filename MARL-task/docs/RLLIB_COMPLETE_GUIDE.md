# RLlibå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ å®Œæ•´æŒ‡å—

**Ray RLlib 2.50.1 - ä»å…¥é—¨åˆ°ç²¾é€š**

---

## ğŸ“‹ ç›®å½•

- [ç¬¬ä¸€éƒ¨åˆ†ï¼šRLlibæ¡†æ¶åŸç†](#ç¬¬ä¸€éƒ¨åˆ†rllibæ¡†æ¶åŸç†)
- [ç¬¬äºŒéƒ¨åˆ†ï¼šç¯å¢ƒå®ç°](#ç¬¬äºŒéƒ¨åˆ†ç¯å¢ƒå®ç°)
- [ç¬¬ä¸‰éƒ¨åˆ†ï¼šè®­ç»ƒé…ç½®](#ç¬¬ä¸‰éƒ¨åˆ†è®­ç»ƒé…ç½®)
- [ç¬¬å››éƒ¨åˆ†ï¼šå¹¶è¡Œä¼˜åŒ–](#ç¬¬å››éƒ¨åˆ†å¹¶è¡Œä¼˜åŒ–)
- [ç¬¬äº”éƒ¨åˆ†ï¼šåˆ†å¸ƒå¼è®­ç»ƒ](#ç¬¬äº”éƒ¨åˆ†åˆ†å¸ƒå¼è®­ç»ƒ)
- [ç¬¬å…­éƒ¨åˆ†ï¼šå®æˆ˜æ¡ˆä¾‹](#ç¬¬å…­éƒ¨åˆ†å®æˆ˜æ¡ˆä¾‹)

---

## ç¬¬ä¸€éƒ¨åˆ†ï¼šRLlibæ¡†æ¶åŸç†

### 1.1 RLlibæ¶æ„æ¦‚è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     RLlibå®Œæ•´æ¶æ„                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ç”¨æˆ·å±‚: Pythonè„šæœ¬ / CLI / Tune API
   â†“
ç®—æ³•å±‚: PPO / SAC / DQN / A3C / APPO
   â†“
æ‰§è¡Œå±‚: Rollout Workers / Training Iterator / Evaluation
   â†“
ç¯å¢ƒå±‚: Gymnasium / MultiAgentEnv / Custom Env
   â†“
åŸºç¡€å±‚: Ray Core (Actor / Object Store / Task Queue)
```

### 1.2 è®­ç»ƒå¾ªç¯

```python
# RLlibè®­ç»ƒå¾ªç¯ä¼ªä»£ç 
class Algorithm:
    def train(self):
        # 1. å¹¶è¡Œé‡‡æ ·
        samples = []
        for worker in self.workers:
            sample = worker.sample()  # å¹¶è¡Œæ‰§è¡Œ
            samples.append(sample)

        # 2. èšåˆæ•°æ®
        train_batch = concat(samples)

        # 3. GPUè®­ç»ƒ
        for minibatch in shuffle(train_batch):
            loss = self.compute_loss(minibatch)
            self.optimizer.step(loss)

        # 4. æ›´æ–°ç­–ç•¥
        self.update_target_networks()

        return metrics
```

### 1.3 Workeræ¶æ„

```
Driver Process (ä¸»è¿›ç¨‹)
    â”‚
    â”œâ”€â”€ Worker 0 (local)  â†’ Env 0
    â”œâ”€â”€ Worker 1 (remote) â†’ Env 1
    â””â”€â”€ Worker N (remote) â†’ Env N

æ¯ä¸ªWorker:
- ç‹¬ç«‹ç¯å¢ƒå®ä¾‹
- ç‹¬ç«‹ç­–ç•¥å‰¯æœ¬ (inferenceæ¨¡å¼)
- æ”¶é›†æ ·æœ¬æ•°æ®
- ä¸è¿›è¡Œæ¢¯åº¦è®¡ç®—
```

---

## ç¬¬äºŒéƒ¨åˆ†ï¼šç¯å¢ƒå®ç°

### 2.1 MultiAgentEnvåŸºç±»

```python
from ray.rllib.env.multi_agent_env import MultiAgentEnv
from gymnasium.spaces import Box
import numpy as np

class BerthAllocationMultiAgentEnv(MultiAgentEnv):
    """æ³Šä½åˆ†é…å¤šæ™ºèƒ½ä½“ç¯å¢ƒ"""

    def __init__(self, config):
        super().__init__()

        # ç¯å¢ƒå‚æ•°
        self.num_vessels = config.get("num_vessels", 10)
        self.berth_length = config.get("berth_length", 2000.0)

        # è§‚æµ‹ç©ºé—´: 17ç»´è¿ç»­
        self.observation_space = Box(
            low=-1.0, high=1.0, shape=(17,), dtype=np.float32
        )

        # åŠ¨ä½œç©ºé—´: 3ç»´è¿ç»­
        self.action_space = Box(
            low=-1.0, high=1.0, shape=(3,), dtype=np.float32
        )

        self._agent_ids = set()

    def reset(self, *, seed=None, options=None):
        """é‡ç½®ç¯å¢ƒ"""
        self._agent_ids = {f"vessel_{i}" for i in range(self.num_vessels)}

        observations = {
            agent_id: self._get_obs(agent_id)
            for agent_id in self._agent_ids
        }
        infos = {agent_id: {} for agent_id in self._agent_ids}

        return observations, infos

    def step(self, action_dict):
        """æ‰§è¡Œä¸€æ­¥"""
        observations = {}
        rewards = {}
        terminateds = {}
        truncateds = {}
        infos = {}

        for agent_id, action in action_dict.items():
            obs, reward, done, truncated, info = self._step_agent(
                agent_id, action
            )
            observations[agent_id] = obs
            rewards[agent_id] = reward
            terminateds[agent_id] = done
            truncateds[agent_id] = truncated
            infos[agent_id] = info

        # å…¨å±€ç»ˆæ­¢æ¡ä»¶
        terminateds["__all__"] = all(terminateds.values())
        truncateds["__all__"] = all(truncateds.values())

        return observations, rewards, terminateds, truncateds, infos
```

### 2.2 ç¯å¢ƒæ³¨å†Œ

```python
from ray.tune.registry import register_env

def register_berth_env():
    def env_creator(env_config):
        return BerthAllocationMultiAgentEnv(env_config)

    register_env("berth_allocation", env_creator)
```

### 2.3 å¤šæ™ºèƒ½ä½“ç­–ç•¥æ¨¡å¼

**æ¨¡å¼1: å…±äº«ç­–ç•¥** (å‚æ•°å…±äº«,è®­ç»ƒå¿«)
```python
config.multi_agent(
    policies={
        "shared_policy": (None, obs_space, act_space, {})
    },
    policy_mapping_fn=lambda agent_id, *args, **kwargs: "shared_policy"
)
```

**æ¨¡å¼2: ç‹¬ç«‹ç­–ç•¥** (æœ€å¤§çµæ´»æ€§)
```python
config.multi_agent(
    policies={
        f"policy_{i}": (None, obs_space, act_space, {})
        for i in range(num_agents)
    },
    policy_mapping_fn=lambda agent_id, *args, **kwargs: f"policy_{agent_id}"
)
```

**æ¨¡å¼3: åˆ†ç»„ç­–ç•¥** (å¹³è¡¡)
```python
def policy_mapping_fn(agent_id, episode, worker, **kwargs):
    agent_idx = int(agent_id.split("_")[1])
    return "group_a" if agent_idx < 5 else "group_b"

config.multi_agent(
    policies={
        "group_a": (None, obs_space, act_space, {}),
        "group_b": (None, obs_space, act_space, {}),
    },
    policy_mapping_fn=policy_mapping_fn
)
```

---

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šè®­ç»ƒé…ç½®

### 3.1 åŸºç¡€é…ç½®æ¨¡æ¿

```python
from ray.rllib.algorithms.sac import SACConfig

# åˆ›å»ºé…ç½®
config = SACConfig()

# ç¯å¢ƒé…ç½®
config.environment(
    env="berth_allocation",
    env_config={
        "num_vessels": 50,
        "planning_horizon_days": 7,
        "berth_length": 2000.0,
    }
)

# æ¡†æ¶é…ç½®
config.framework("torch")

# èµ„æºé…ç½®
config.resources(
    num_gpus=1,
    num_cpus_for_driver=2,
)

# Rollouté…ç½®
config.rollouts(
    num_rollout_workers=8,
    num_envs_per_worker=2,
    rollout_fragment_length=500,
)

# è®­ç»ƒé…ç½®
config.training(
    train_batch_size=8000,
    replay_buffer_config={"capacity": 500000},
    optimization={
        "actor_learning_rate": 3e-4,
        "critic_learning_rate": 3e-4,
    },
)

# è¯„ä¼°é…ç½®
config.evaluation(
    evaluation_interval=10,
    evaluation_duration=10,
)

# æ„å»ºç®—æ³•
algo = config.build()
```

### 3.2 SACç®—æ³•é…ç½®

```python
config = SACConfig()

config.training(
    train_batch_size=8000,
    replay_buffer_config={
        "type": "MultiAgentReplayBuffer",
        "capacity": 500000,
    },
    optimization={
        "actor_learning_rate": 3e-4,
        "critic_learning_rate": 3e-4,
        "entropy_learning_rate": 3e-4,
    },
    tau=0.005,
    target_network_update_freq=1,
    twin_q=True,
    policy_delay=2,
)
```

### 3.3 PPOç®—æ³•é…ç½®

```python
from ray.rllib.algorithms.ppo import PPOConfig

config = PPOConfig()

config.training(
    train_batch_size=8000,
    sgd_minibatch_size=256,
    num_sgd_iter=10,
    lr=3e-4,
    gamma=0.99,
    lambda_=0.95,
    clip_param=0.2,
    vf_clip_param=10.0,
    entropy_coeff=0.01,
)
```

### 3.4 å®Œæ•´è®­ç»ƒè„šæœ¬

```python
#!/usr/bin/env python3
import ray
from ray.rllib.algorithms.sac import SACConfig

# åˆå§‹åŒ–Ray
ray.init(num_gpus=1)

# æ³¨å†Œç¯å¢ƒ
register_berth_env()

# é…ç½®
config = SACConfig()
config.environment(env="berth_allocation", env_config={"num_vessels": 50})
config.resources(num_gpus=1)
config.rollouts(num_rollout_workers=8)

# è®­ç»ƒ
algo = config.build()
best_reward = -float('inf')

for i in range(1000):
    result = algo.train()

    reward = result['episode_reward_mean']
    print(f"Iteration {i}: Reward={reward:.2f}")

    if reward > best_reward:
        best_reward = reward
        checkpoint = algo.save()
        print(f"  Saved checkpoint: {checkpoint}")

algo.stop()
ray.shutdown()
```

---

## ç¬¬å››éƒ¨åˆ†ï¼šå¹¶è¡Œä¼˜åŒ–

### 4.1 æ•°æ®å¹¶è¡Œ (æ ¸å¿ƒæœºåˆ¶)

```python
# æ•°æ®å¹¶è¡Œé…ç½®
config.rollouts(
    num_rollout_workers=16,      # 16ä¸ªworkerå¹¶è¡Œé‡‡æ ·
    num_envs_per_worker=2,       # æ¯worker 2ç¯å¢ƒ
    rollout_fragment_length=500, # æ¯æ¬¡500æ­¥
)

# æ•°æ®æµ:
# 16 workers Ã— 2 envs Ã— 500 steps = 16000 æ ·æœ¬/è¿­ä»£
```

**å·¥ä½œæµç¨‹**:
```
æ—¶é—´æ­¥ t:
Worker 1-16 å¹¶è¡Œé‡‡æ · â†’ èšåˆ16000æ ·æœ¬ â†’ GPUè®­ç»ƒ â†’ æ›´æ–°ç­–ç•¥
```

### 4.2 è‡ªåŠ¨èµ„æºä¼˜åŒ–

```python
import multiprocessing
import torch

def get_optimal_config():
    """è‡ªåŠ¨è®¡ç®—æœ€ä¼˜é…ç½®"""
    num_cpus = multiprocessing.cpu_count()
    num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0

    # è®¡ç®—æœ€ä¼˜workeræ•°
    if num_gpus > 0:
        num_workers = min(num_cpus - 2, num_gpus * 8)
    else:
        num_workers = max(2, num_cpus // 2)

    # è®¡ç®—batchå¤§å°
    num_envs = 2
    fragment_length = 500 if num_gpus > 0 else 200
    train_batch_size = num_workers * num_envs * fragment_length

    return {
        "num_cpus": num_cpus,
        "num_gpus": num_gpus,
        "num_workers": num_workers,
        "train_batch_size": train_batch_size,
    }

# ä½¿ç”¨
optimal = get_optimal_config()
config.resources(num_gpus=optimal["num_gpus"])
config.rollouts(num_rollout_workers=optimal["num_workers"])
config.training(train_batch_size=optimal["train_batch_size"])
```

### 4.3 GPUä¼˜åŒ–

```python
# å•GPUä¼˜åŒ–
config.resources(
    num_gpus=1,
    num_cpus_for_driver=2,
)
config.training(
    sgd_minibatch_size=512,  # å¤§batchæé«˜GPUåˆ©ç”¨ç‡
    num_sgd_iter=10,         # å¤šæ¬¡è¿­ä»£
)

# å¤šGPU learner
config.resources(num_gpus=0)  # Driverä¸ç”¨GPU
config.learners(
    num_learner_workers=4,
    num_gpus_per_learner_worker=1,
)
```

### 4.4 æ€§èƒ½ä¼˜åŒ–é…ç½®

```python
# é€Ÿåº¦ä¼˜å…ˆé…ç½®
config.rollouts(
    num_rollout_workers=32,
    num_envs_per_worker=4,
    rollout_fragment_length=2000,
    compress_observations=True,  # å‹ç¼©è§‚æµ‹
)
config.training(
    train_batch_size=128000,
    replay_buffer_config={"capacity": 500000},
)

# è´¨é‡ä¼˜å…ˆé…ç½®
config.rollouts(
    num_rollout_workers=8,
    num_envs_per_worker=1,
    rollout_fragment_length=500,
)
config.training(
    train_batch_size=4000,
    replay_buffer_config={"capacity": 2000000},
    num_sgd_iter=20,
)
```

---

## ç¬¬äº”éƒ¨åˆ†ï¼šåˆ†å¸ƒå¼è®­ç»ƒ

### 5.1 å•æœºå¤šGPU

```python
# 4Ã—GPUé…ç½®
import ray
ray.init(num_gpus=4, num_cpus=32)

config = SACConfig()
config.environment(env="berth_allocation", env_config={"num_vessels": 100})

config.resources(num_gpus=0, num_cpus_for_driver=8)
config.rollouts(
    num_rollout_workers=24,
    num_envs_per_worker=4,
    rollout_fragment_length=1000,
)
config.training(train_batch_size=48000)
config.learners(
    num_learner_workers=4,
    num_gpus_per_learner_worker=1,
)

algo = config.build()
```

### 5.2 Rayé›†ç¾¤è®­ç»ƒ

**å¯åŠ¨é›†ç¾¤**:
```bash
# HeadèŠ‚ç‚¹ (192.168.1.100)
ray start --head --port=6379 --dashboard-host=0.0.0.0

# WorkerèŠ‚ç‚¹
ray start --address='192.168.1.100:6379'

# æŸ¥çœ‹é›†ç¾¤
ray status
```

**æäº¤è®­ç»ƒ**:
```python
import ray

# è¿æ¥é›†ç¾¤
ray.init(address='auto')

config = SACConfig()
config.environment(env="berth_allocation", env_config={"num_vessels": 200})

# é›†ç¾¤èµ„æºé…ç½®
config.resources(num_gpus=8, num_cpus_for_driver=16)
config.rollouts(
    num_rollout_workers=64,
    num_envs_per_worker=4,
    rollout_fragment_length=2000,
    remote_worker_envs=True,
)
config.training(train_batch_size=256000)

algo = config.build()
for i in range(10000):
    result = algo.train()
```

### 5.3 èµ„æºåˆ†é…ç­–ç•¥

```python
# CPUå¯†é›†å‹ (ç¯å¢ƒå¤æ‚)
config.resources(
    num_gpus=1,
    num_cpus_per_worker=2,  # æ¯worker 2 CPU
)

# GPUå¯†é›†å‹ (æ¨¡å‹å¤§)
config.resources(
    num_gpus=4,
    num_cpus_per_worker=1,
    num_gpus_per_worker=0.1,  # Workerå…±äº«GPU
)

# æ··åˆå‹
config.resources(
    num_gpus=2,
    num_cpus_per_worker=1.5,
)
config.learners(
    num_learner_workers=2,
    num_gpus_per_learner_worker=1,
)
```

---

## ç¬¬å…­éƒ¨åˆ†ï¼šå®æˆ˜æ¡ˆä¾‹

### 6.1 æœ¬åœ°å¼€å‘ (Macç¬”è®°æœ¬)

```python
config = SACConfig()
config.environment(env="berth_allocation", env_config={"num_vessels": 10})
config.resources(num_gpus=0, num_cpus_for_driver=1)
config.rollouts(num_rollout_workers=2, rollout_fragment_length=200)
config.training(train_batch_size=400, replay_buffer_config={"capacity": 10000})

# é¢„æœŸ: ~5åˆ†é’Ÿ/100è¿­ä»£, ~2GBå†…å­˜
```

### 6.2 å•GPUå·¥ä½œç«™

```python
config = SACConfig()
config.environment(env="berth_allocation", env_config={"num_vessels": 50})
config.resources(num_gpus=1, num_cpus_for_driver=2)
config.rollouts(num_rollout_workers=8, num_envs_per_worker=2, rollout_fragment_length=500)
config.training(train_batch_size=8000, replay_buffer_config={"capacity": 100000})

# é¢„æœŸ: ~2å°æ—¶/1000è¿­ä»£, GPUåˆ©ç”¨ç‡70%, 8000æ ·æœ¬/ç§’
```

### 6.3 å¤šGPUæœåŠ¡å™¨ (4Ã—GPU)

```python
config = SACConfig()
config.environment(env="berth_allocation", env_config={"num_vessels": 100})
config.resources(num_gpus=4, num_cpus_for_driver=8)
config.rollouts(
    num_rollout_workers=32,
    num_envs_per_worker=4,
    rollout_fragment_length=1000,
    remote_worker_envs=True,
)
config.training(train_batch_size=32000)
config.learners(num_learner_workers=4, num_gpus_per_learner_worker=1)

# é¢„æœŸ: ~12å°æ—¶/10000è¿­ä»£, GPUåˆ©ç”¨ç‡85%, 30000æ ·æœ¬/ç§’
```

### 6.4 Rayé›†ç¾¤ (10èŠ‚ç‚¹)

```python
ray.init(address='auto')

config = SACConfig()
config.environment(env="berth_allocation", env_config={"num_vessels": 200})
config.resources(num_gpus=8, num_cpus_for_driver=16)
config.rollouts(
    num_rollout_workers=64,
    num_envs_per_worker=4,
    rollout_fragment_length=2000,
    remote_worker_envs=True,
)
config.training(train_batch_size=64000)
config.learners(num_learner_workers=8, num_gpus_per_learner_worker=1)

# é¢„æœŸ: ~24å°æ—¶/100000è¿­ä»£, é›†ç¾¤GPUåˆ©ç”¨ç‡80%, 100000æ ·æœ¬/ç§’
```

### 6.5 æ€§èƒ½å¯¹æ¯”

| é…ç½® | ç¡¬ä»¶ | æ—¶é—´(100è‰˜èˆ¹,5000è¿­ä»£) | åŠ é€Ÿæ¯” |
|------|------|---------------------|--------|
| å•æœºCPU | 8æ ¸ | 48h | 1Ã— |
| å•æœºGPU | 8æ ¸+1GPU | 18h | 2.7Ã— |
| å•æœº4GPU | 32æ ¸+4GPU | 6h | 8Ã— |
| 3èŠ‚ç‚¹é›†ç¾¤ | 32æ ¸+4GPUÃ—3 | 2.5h | 19Ã— |
| 10èŠ‚ç‚¹é›†ç¾¤ | 384æ ¸+48GPU | 1h | 48Ã— |

---

## é™„å½•Aï¼šå¿«é€Ÿå‘½ä»¤å‚è€ƒ

### è®­ç»ƒå‘½ä»¤

```bash
# åŸºç¡€è®­ç»ƒ
python rllib_train.py --algo SAC --num-vessels 50 --iterations 1000

# è‡ªåŠ¨ä¼˜åŒ–
python rllib_train_advanced.py --auto-resources --optimize-for speed

# GPUè®­ç»ƒ
python rllib_train_advanced.py --gpus 2 --workers 16

# é›†ç¾¤è®­ç»ƒ
python rllib_train_advanced.py --distributed --auto-resources

# æ€§èƒ½åˆ†æ
python rllib_train_advanced.py --profile --local
```

### Rayé›†ç¾¤å‘½ä»¤

```bash
# å¯åŠ¨head
ray start --head --port=6379 --dashboard-host=0.0.0.0

# å¯åŠ¨worker
ray start --address='HEAD_IP:6379'

# æŸ¥çœ‹çŠ¶æ€
ray status

# åœæ­¢Ray
ray stop
```

### ç›‘æ§å‘½ä»¤

```bash
# Ray Dashboard
open http://localhost:8265

# TensorBoard
tensorboard --logdir=~/ray_results --port=6006
```

---

## é™„å½•Bï¼šæ€§èƒ½è°ƒä¼˜æ¸…å•

### å¿…åšä¼˜åŒ–
- [x] è®¾ç½®åˆç†workeræ•°é‡ (CPUæ ¸æ•°-2)
- [x] é…ç½®é€‚å½“batchå¤§å° (workers Ã— envs Ã— fragment)
- [x] å¯ç”¨GPUåŠ é€Ÿ (å¦‚æœ‰)
- [x] å¯ç”¨è§‚æµ‹å‹ç¼©
- [x] å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹

### æ¨èä¼˜åŒ–
- [ ] è°ƒæ•´å­¦ä¹ ç‡
- [ ] ä¼˜åŒ–replay bufferå¤§å°
- [ ] ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
- [ ] é…ç½®å¤šGPU learner
- [ ] ä¼˜åŒ–ç¯å¢ƒä»£ç 

### é«˜çº§ä¼˜åŒ–
- [ ] è‡ªå®šä¹‰ç½‘ç»œæ¶æ„
- [ ] å®ç°è‡ªå®šä¹‰é‡‡æ ·ç­–ç•¥
- [ ] ä½¿ç”¨å¼‚æ­¥è®­ç»ƒ(APPO)
- [ ] å®ç°æ¨¡å‹å‹ç¼©

---

## é™„å½•Cï¼šæ•…éšœæ’æŸ¥

### é—®é¢˜1: Workerè¿æ¥è¶…æ—¶
```python
# å¢åŠ workeræ•°é‡æˆ–å‡å°‘å¯åŠ¨è¶…æ—¶
config.rollouts(num_rollout_workers=8)  # ä»16å‡åˆ°8
```

### é—®é¢˜2: GPUå†…å­˜ä¸è¶³
```python
# å‡å°‘batchå¤§å°
config.training(sgd_minibatch_size=128)  # ä»512å‡åˆ°128
```

### é—®é¢˜3: Object Storeæ»¡
```bash
# é‡å¯Ray,å¢åŠ object store
ray stop
ray start --head --object-store-memory=100000000000
```

### é—®é¢˜4: å¥–åŠ±ä¸æ”¶æ•›
```python
# é™ä½å­¦ä¹ ç‡,å¢åŠ è®­ç»ƒæ—¶é—´
config.training(
    optimization={"actor_learning_rate": 1e-4}  # ä»3e-4é™åˆ°1e-4
)
```

---

## é™„å½•Dï¼šæ€§èƒ½ç›®æ ‡

| è§„æ¨¡ | èˆ¹èˆ¶æ•° | ååé‡ | è®­ç»ƒæ—¶é—´(1000è¿­ä»£) |
|------|--------|--------|------------------|
| å°è§„æ¨¡ | 10 | 2Kæ ·æœ¬/ç§’ | <1å°æ—¶ |
| ä¸­è§„æ¨¡ | 50 | 10Kæ ·æœ¬/ç§’ | <6å°æ—¶ |
| å¤§è§„æ¨¡ | 100 | 30Kæ ·æœ¬/ç§’ | <12å°æ—¶ |
| è¶…å¤§è§„æ¨¡ | 200+ | 100Kæ ·æœ¬/ç§’ | <24å°æ—¶ |

**GPUåˆ©ç”¨ç‡ç›®æ ‡**:
- å•GPU: 70-85%
- å¤šGPU (2-4): 75-90%
- é›†ç¾¤ (8+): 65-80%

---

**æ–‡æ¡£ç»´æŠ¤**: Duan
**æœ€åæ›´æ–°**: 2025-10-28
**RLlibç‰ˆæœ¬**: 2.50.1
